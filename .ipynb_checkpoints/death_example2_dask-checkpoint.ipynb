{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ad12bdc-66e8-49ae-be6f-333a4cac80b1",
   "metadata": {},
   "source": [
    "# check output files with dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a99e78d7-570e-4e3b-b90f-4b9c36561646",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.matlib\n",
    "import numpy.ma as ma\n",
    "\n",
    "import xarray as xr\n",
    "time_coder = xr.coders.CFDatetimeCoder(use_cftime=True) #create time coder with cftime\n",
    "\n",
    "import time\n",
    "import cftime\n",
    "import netCDF4 as nc\n",
    "from datetime import timedelta\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import glob\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19700307-6ecc-4fa5-a3a7-a32f091ee05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load custom functions for analyzing flat10\n",
    "\n",
    "from loading_function_flat10 import load_flat10, load_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f43338-2e80-4fda-9baa-e4af0465ce87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "from dask_jobqueue import PBSCluster\n",
    "from dask.distributed import Client\n",
    "\n",
    "################\n",
    "##### Dask #####\n",
    "################\n",
    "\n",
    "def get_ClusterClient(\n",
    "        ncores=1,\n",
    "        nmem='200GB',\n",
    "        walltime='01:00:00',\n",
    "        account='UWAS0155'):\n",
    "    \"\"\"\n",
    "    Code from Daniel Kennedy\n",
    "    More info about Dask on HPC - https://ncar.github.io/dask-tutorial/notebooks/05-dask-hpc.html\n",
    "    \"\"\"\n",
    "    cluster = PBSCluster(\n",
    "        cores=ncores,              # The number of cores you want\n",
    "        memory=nmem,               # Amount of memory\n",
    "        processes=ncores,          # How many processes\n",
    "        queue='casper',            # Queue name\n",
    "        resource_spec='select=1:ncpus=' +\\\n",
    "        str(ncores)+':mem='+nmem,  # Specify resources\n",
    "        account=account,           # Input your project ID here\n",
    "        walltime=walltime,         # Amount of wall time\n",
    "        interface='ext',           # Interface to use\n",
    "    )\n",
    "\n",
    "    client = Client(cluster)\n",
    "    return cluster, client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1b00c56-7823-4504-ae59-28bebb6090d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict={}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca3430e-6e09-4108-91f5-528d2e350992",
   "metadata": {},
   "source": [
    "# Zonal correction for NorESM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9550009e-66d5-4f87-ae72-07ee3dc64030",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputdir= '/glade/campaign/cgd/tss/people/aswann/flat10/'\n",
    "\n",
    "# modellist_orig= ['ACCESS-ESM1-5',  \n",
    "#             'CESM2',    \n",
    "#             'GFDL-ESM4',  \n",
    "#             'GISS_E2.1',  \n",
    "#             'NorESM2-LM',\n",
    "#             'MPI-ESM1-2-LR',\n",
    "#             'CNRM-ESM2-1',\n",
    "#             'HadCM3LC-Bris']\n",
    "modellist=['NorESM2-LM']\n",
    "\n",
    "runlist = ['flat10','flat10_zec','flat10_cdr']\n",
    "#runlist = ['flat10-cdr']\n",
    "# use a wildcard to capture different ways the folders and runs are named across models\n",
    "runlist_wc = ['*lat10','*zec','*cdr']\n",
    "#runlist_wc = ['*cdr']\n",
    "\n",
    "varlist_load=['cVeg','cSoil','cLitter','nbp','gpp','rh'] #, 'gpp','fgco2', 'ra', 'rh']#, 'npp'] # not working beyond nbp for norESM\n",
    "varlist_analyze=['cVeg','cSoil','cTot','cLitter','nbp','gpp','rh']\n",
    "varlist=varlist_load\n",
    "#varlist=['rh']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd7144c-f408-4df8-9f9e-e92e148c3f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create Dask cluster and client, scale up to 20 workers\n",
    "cluster, client = get_ClusterClient(walltime='02:00:00')\n",
    "cluster.scale(20)\n",
    "client.wait_for_workers(20)\n",
    "\n",
    "## Lists active workers and their status\n",
    "cluster.workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6d56b19-825f-48cf-81b8-4fa6aa8f006f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NorESM2-LM getting grid info\n"
     ]
    }
   ],
   "source": [
    "# load grid\n",
    "data_dict = load_grid(data_dict,modellist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a0e1dab1-af3d-408a-86aa-6bb3053c7c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all data\n",
    "##data_dict=load_flat10(data_dict, modellist, runlist, runlist_wc, varlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7f870a-fb28-4df6-bee5-59f1206b0cb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading model: NorESM2-LM\n",
      "loading run: *lat10\n",
      "loading variable: cVeg\n",
      "loading variable: cSoil\n",
      "loading variable: cLitter\n",
      "loading variable: nbp\n",
      "loading variable: gpp\n",
      "loading variable: rh\n",
      "adding NorESM2-LM flat10 to dict\n",
      "loading run: *zec\n",
      "loading variable: cVeg\n",
      "loading variable: cSoil\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import numpy.matlib\n",
    "import numpy.ma as ma\n",
    "\n",
    "import xarray as xr\n",
    "#xr.set_options(enable_cftimeindex=True)\n",
    "#from xarray.coding.times import CFTimedeltaCoder\n",
    "time_coder = xr.coders.CFDatetimeCoder(use_cftime=True) #create time coder with cftime\n",
    "\n",
    "import time\n",
    "import cftime\n",
    "import netCDF4 as nc\n",
    "from datetime import timedelta\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import glob\n",
    "\n",
    "\n",
    "## notes on packages to add to this kernel\n",
    "import nc_time_axis\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# data location\n",
    "outputdir= '/glade/campaign/cgd/tss/people/aswann/flat10/'\n",
    "\n",
    "#----loop over models----#\n",
    "for m in range(len(modellist)):\n",
    "#for m in range(len(['GFDL-ESM4',  'GISS_E2.1',  'NorESM2-LM','MPI-ESM1-2-LR'])):\n",
    "    model=modellist[m]\n",
    "    print('loading model: ' +model)\n",
    "    #----loop over experiments----# \n",
    "    for r in range(len(runlist)):\n",
    "        run = runlist_wc[r]\n",
    "        print('loading run: ' +run)\n",
    "        #----loop over variables----#\n",
    "        for v in range(len(varlist)):\n",
    "            var=varlist[v]\n",
    "            print('loading variable: ' +var)\n",
    "            \n",
    "            searchpath= outputdir +model +'/' +run +'/*' +var +'_*.nc'\n",
    "            \n",
    "            filenamelist= np.sort(glob.glob(searchpath)) # sort in time order, xarray was having trouble arranging some of them in time dim\n",
    "\n",
    "            #----loop over filenames----#\n",
    "            # some variables are stored in multiple files\n",
    "            # this should be possible with xr.open_mfdataset but it isn't loading all of time points\n",
    "            for f in range(len(filenamelist)):\n",
    "                file = filenamelist[f]\n",
    "                if f==0:\n",
    "                    dsmerge_f = xr.open_dataset(file,decode_times=time_coder)\n",
    "                else:\n",
    "                    ds = xr.open_dataset(file,decode_times=time_coder)\n",
    "                    dsmerge_f=xr.concat([dsmerge_f,ds],dim='time')\n",
    "\n",
    "            \n",
    "            if model == 'NorESM2-LM':\n",
    "                if 'PRECC' in dsmerge_f: #NorESM\n",
    "                    dsmerge_f['pr']=dsmerge_f['PRECC']\n",
    "                    if dsmerge_f['pr'].units == 'm/s':\n",
    "                        dsmerge_f['pr']=dsmerge_f['pr']*(1e3)\n",
    "                        dsmerge_f['pr'].attrs['units'] = 'kg m-2 s-1' #equivalent is mm/s\n",
    "                \n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "            #----check units and convert if necessary----#\n",
    "            if var in dsmerge_f: \n",
    "                if model =='CESM2':\n",
    "                    if dsmerge_f[var].units == 'gC/m^2/s':\n",
    "                        dsmerge_f[var]=dsmerge_f[var]*(1/1000) # convert from gC to kgC\n",
    "                        dsmerge_f[var].attrs['units'] = 'kg m-2 s-1'\n",
    "                    # stock variables\n",
    "\n",
    "                \n",
    "            else: #var does not exist\n",
    "                ds=dsmerge_f\n",
    "                # add a blank variable so that loops work\n",
    "                if 'time' in ds:\n",
    "                    nan_dataarray = xr.DataArray(np.full((len(ds['time']),len(ds['lat']), len(ds['lon'])), np.nan), \n",
    "                                                 coords={'lon': ds['lon'], 'lat': ds['lat'],'time': ds['time']}, dims=['time','lat', 'lon'])\n",
    "\n",
    "                # Assign the new variable to the dataset\n",
    "                dsmerge_f[var] = nan_dataarray\n",
    "            \n",
    "            #----merge all variables into one dataset----#\n",
    "            # if it's the first variable, then start a new datset, otherwise merge with existing\n",
    "            if v ==0:\n",
    "                dsmerge_v = dsmerge_f.copy()\n",
    "            else:\n",
    "                dsmerge_v=xr.merge([dsmerge_v, dsmerge_f],compat='override')\n",
    "\n",
    "            # add a new variable that is the sum of all carbon pools\n",
    "            if all(var_name in dsmerge_v for var_name in ['cVeg', 'cSoil', 'cLitter']):\n",
    "                if (dsmerge_v['cLitter'].notnull().all()): #litter is sometimes missing. Would be good to make this more general but dealing with this problem for now.\n",
    "                    dsmerge_v['cTot'] = dsmerge_v['cVeg']+dsmerge_v['cSoil']+dsmerge_v['cLitter'] \n",
    "                else: \n",
    "                    dsmerge_v['cTot'] = dsmerge_v['cVeg']+dsmerge_v['cSoil'] \n",
    "        \n",
    "        #----save output to a dictionary----#\n",
    "        print('adding ' +model +' ' +runlist[r] +' to dict')\n",
    "        data_dict[model +'_' +runlist[r]] = dsmerge_v\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847c7eaf-768a-43e6-92bf-96f1c3308144",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4a3188-ef52-4616-a1a6-e2dc13b6f0b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71ab567-12fd-426f-b22e-ab3337861b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Once done, shut down the Dask cluster\n",
    "client.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08d2e34-d681-4e5f-bcec-d8ef6e0e6cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm ./dask-worker.e*\n",
    "!rm ./dask-worker.o*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NPL 2025b",
   "language": "python",
   "name": "npl-2025b"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
