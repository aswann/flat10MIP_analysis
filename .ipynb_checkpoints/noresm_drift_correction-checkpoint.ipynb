{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77a79026-91c6-47fc-9568-1e2194f451b9",
   "metadata": {},
   "source": [
    "# Apply drift correction to NorESM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe833992-dd45-42c8-b7f3-39dba265c2ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.matlib\n",
    "import numpy.ma as ma\n",
    "\n",
    "import xarray as xr\n",
    "#xr.set_options(enable_cftimeindex=True)\n",
    "\n",
    "import time\n",
    "import cftime\n",
    "import netCDF4 as nc\n",
    "from datetime import timedelta\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "#import xcdat\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#import cartopy.crs as ccrs\n",
    "#from cartopy.util import add_cyclic_point\n",
    "\n",
    "import glob\n",
    "\n",
    "\n",
    "## notes on packages to add to this kernel\n",
    "import nc_time_axis\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "214e5dd7-fbc1-41fe-9462-63dc34f8559b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load custom functions for analyzing flat10\n",
    "\n",
    "from loading_function_flat10 import load_flat10, load_grid, select_time_slice, weighted_temporal_mean "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "133b8139-9f4f-4fcd-a91f-8f36aa746be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "from dask_jobqueue import PBSCluster\n",
    "from dask.distributed import Client\n",
    "\n",
    "################\n",
    "##### Dask #####\n",
    "################\n",
    "\n",
    "def get_ClusterClient(\n",
    "        ncores=1,\n",
    "        nmem='200GB',\n",
    "        walltime='01:00:00',\n",
    "        account='UWAS0155'):\n",
    "    \"\"\"\n",
    "    Code from Daniel Kennedy\n",
    "    More info about Dask on HPC - https://ncar.github.io/dask-tutorial/notebooks/05-dask-hpc.html\n",
    "    \"\"\"\n",
    "    cluster = PBSCluster(\n",
    "        cores=ncores,              # The number of cores you want\n",
    "        memory=nmem,               # Amount of memory\n",
    "        processes=ncores,          # How many processes\n",
    "        queue='casper',            # Queue name\n",
    "        resource_spec='select=1:ncpus=' +\\\n",
    "        str(ncores)+':mem='+nmem,  # Specify resources\n",
    "        account=account,           # Input your project ID here\n",
    "        walltime=walltime,         # Amount of wall time\n",
    "        interface='ext',           # Interface to use\n",
    "    )\n",
    "\n",
    "    client = Client(cluster)\n",
    "    return cluster, client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e42a56-df71-4b0b-b51e-9fbbc1ff390c",
   "metadata": {},
   "source": [
    "### Data location\n",
    "\n",
    "Data downloaded for nird.sigma2.no to /glade/campaign/cgd/tss/people/aswann/flat10/\n",
    "\n",
    "#### Models\n",
    "Models with output:  \n",
    "ACCESS-ESM1-5  \n",
    "CESM2  \n",
    "CNRM-ESM2-1  \n",
    "GFDL-ESM4  \n",
    "GISS_E2.1  \n",
    "NorESM2-LM  \n",
    "MPI-ESM1-2-LR \n",
    "CNRM-ESM2-1  \n",
    "HadCM3LC-Bris  \n",
    "UKESM1.2  \n",
    "\n",
    "Directory structures within each model folder vary - need to account for this \n",
    "\n",
    "#### Experiments\n",
    "flat10: 200 years at 10Pg/yr  \n",
    "flat10_zec: branches from flat10 at 100 years, zero emissions for 200 years  \n",
    "flat10_cdr: branches from flat10 at 100 years, ramps down to negative 10 Pg/yr over 100 years, continues at zero emissions for 100 years (until 200 years past flat10)\n",
    "\n",
    "Total length of each simulation on its own: 200 years\n",
    "\n",
    "#### Time indexing\n",
    "Each model did it's time indexing in a different way. Need to account for all of the different calendars.\n",
    "\n",
    "#### Missing files\n",
    "The catalog of all CMIP files on glade is at:  \n",
    "catalog_file = '/glade/collections/cmip/catalog/intake-esm-datastore/catalogs/glade-cmip6.csv.gz'\n",
    "\n",
    "GISS is missing the land fraction data (sftlf) and cell area data (areacella, areacello)\n",
    "I tried to find matching grid files on NCAR, but none of the GISS models matched in resolution\n",
    "Ended up downloading grid files from PCMDI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "696570b8-1a29-4594-87c2-5ac8420f2f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputdir= '/glade/campaign/cgd/tss/people/aswann/flat10/'\n",
    "\n",
    "# modellist_orig= ['ACCESS-ESM1-5',  \n",
    "#             'CESM2',    \n",
    "#             'GFDL-ESM4',  \n",
    "#             'GISS_E2.1',  \n",
    "#             'NorESM2-LM',\n",
    "#             'MPI-ESM1-2-LR',\n",
    "#             'CNRM-ESM2-1',\n",
    "#             'HadCM3LC-Bris',\n",
    "#             'UKESM1.2']\n",
    "# modellist=modellist_orig\n",
    "\n",
    "runlist = ['flat10','flat10_zec','flat10_cdr']\n",
    "# use a wildcard to capture different ways the folders and runs are named across models\n",
    "runlist_wc = ['*lat10','*zec','*cdr']\n",
    "\n",
    "# varlist_load=['cVeg','cSoil','cLitter']#,'nbp','gpp','rh','tas','pr'] #, 'gpp','fgco2', 'ra', 'rh']#, 'npp'] # not working beyond nbp for norESM\n",
    "# varlist_analyze=['cVeg','cSoil','cTot','cLitter']#,'nbp','gpp','rh','tas','pr']\n",
    "# varlist=varlist_load\n",
    "unitslist=['kgC m-2','kgC m-2','kgC m-2','kgC m-2 s-1','kgC m-2 s-1','kgC m-2 s-1','K','kg m-2 s-1']\n",
    "\n",
    "# there seems to be a problem with ra for NorESM\n",
    "\n",
    "modelcolors=['tab:blue','tab:orange','tab:green','tab:red','tab:gray','tab:purple','tab:cyan','gold','tab:brown']\n",
    "\n",
    "troplat=30\n",
    "highlat=60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e934389e-dd95-407d-a2d9-ce3de129911f",
   "metadata": {},
   "outputs": [],
   "source": [
    "##------ only for NorESM\n",
    "\n",
    "modellist=['NorESM2-LM']\n",
    "#modellist=['UKESM1.2']\n",
    "runlist=['flat10_zec']\n",
    "varlist=['cSoil']#'cVeg']#,'cSoil','cLitter']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d5fdfe1f-8f5f-4b7f-9a13-41dc726d47d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#unitconversions\n",
    "sperday=60*60*24\n",
    "speryr=60*60*24*365\n",
    "\n",
    "PgperKg = 1e-12\n",
    "# to convert kg m-2 s-1 to kg/m2/yr multiply by speryr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b6505e32-b23b-411f-adb1-21e43074dc35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize a dictionary to hold all of the data\n",
    "data_dict={}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d81c8895-17ae-4900-94b7-1f42c6e46356",
   "metadata": {},
   "source": [
    "### Start Dask workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "829ec327-87d7-455c-936c-18265ad1d1a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Create Dask cluster and client, scale up to 20 workers\n",
    "# cluster, client = get_ClusterClient(walltime='01:00:00')\n",
    "# cluster.scale(20)\n",
    "# client.wait_for_workers(20)\n",
    "\n",
    "# ## Lists active workers and their status\n",
    "# cluster.workers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a6ebb4-ee34-48eb-9130-cc6b6c9a00bf",
   "metadata": {},
   "source": [
    "### Load grid info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b8ad47bd-e0ba-4804-a188-d70a4750c163",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NorESM2-LM getting grid info\n"
     ]
    }
   ],
   "source": [
    "data_dict = load_grid(data_dict,modellist)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f015eb07-826f-4119-8538-c69e386d7804",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "51b17a23-6978-41eb-84e3-43954d26a0ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading model: NorESM2-LM\n",
      "loading run: *lat10\n",
      "loading variable: cSoil\n",
      "adding NorESM2-LM flat10_zec to dict\n"
     ]
    }
   ],
   "source": [
    "data_dict=load_flat10(data_dict, modellist, runlist, runlist_wc, varlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ce7c04-ea78-4129-8b53-a504e158e8af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d53b32c8-9762-47dd-9ef6-e44fe86db072",
   "metadata": {},
   "outputs": [],
   "source": [
    "#modellist=modellist_orig\n",
    "#varlist=varlist_analyze"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "764f7c7c-607f-413f-83ff-8d4efe857e81",
   "metadata": {},
   "source": [
    "# Calculate Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b12ff387-6f89-47c5-91f9-d03232d9d8d3",
   "metadata": {},
   "source": [
    "### zonal average land sink\n",
    "\n",
    "Zonal average land sink is calculated from total carbon on land averaged into latitude bands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3a6492a9-3c4c-4af7-962b-93a2c45d60f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing model: NorESM2-LM\n",
      "processing run: flat10_zec\n",
      "processing variable: cSoil\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/glade/campaign/cgd/tss/people/aswann/flat10/NorESM2-LM/NorESM2-LM_2D_TOTSOM_ann_drift.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 65\u001b[39m\n\u001b[32m     62\u001b[39m     ds[var]=ds[var]-adjustment \u001b[38;5;66;03m# remove the drift from the variable\u001b[39;00m\n\u001b[32m     64\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m var==\u001b[33m'\u001b[39m\u001b[33mcSoil\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m65\u001b[39m     field = pickle.load(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m/glade/campaign/cgd/tss/people/aswann/flat10/NorESM2-LM/NorESM2-LM_2D_TOTSOM_ann_drift.pkl\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mrb\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[32m     66\u001b[39m     adj_matrix = xr.DataArray(np.squeeze(field), dims=[\u001b[33m'\u001b[39m\u001b[33mlat\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33mlon\u001b[39m\u001b[33m'\u001b[39m], coords={\u001b[33m'\u001b[39m\u001b[33mlatitude\u001b[39m\u001b[33m'\u001b[39m: ds.lat, \u001b[33m'\u001b[39m\u001b[33mlongitude\u001b[39m\u001b[33m'\u001b[39m:ds.lon})\u001b[38;5;66;03m##,unit={'g C m-2 yr-1'})\u001b[39;00m\n\u001b[32m     67\u001b[39m     ty=ds[\u001b[33m'\u001b[39m\u001b[33mtime\u001b[39m\u001b[33m'\u001b[39m].dt.year\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/glade/u/apps/opt/conda/envs/npl-2025b/lib/python3.12/site-packages/IPython/core/interactiveshell.py:343\u001b[39m, in \u001b[36m_modified_open\u001b[39m\u001b[34m(file, *args, **kwargs)\u001b[39m\n\u001b[32m    336\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m}:\n\u001b[32m    337\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    338\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mIPython won\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m by default \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    339\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    340\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33myou can use builtins\u001b[39m\u001b[33m'\u001b[39m\u001b[33m open.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    341\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m343\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: '/glade/campaign/cgd/tss/people/aswann/flat10/NorESM2-LM/NorESM2-LM_2D_TOTSOM_ann_drift.pkl'"
     ]
    }
   ],
   "source": [
    "# example for one model and one experiment\n",
    "# needs a loop to be broadcast to all runs\n",
    "#m=0\n",
    "#e=0\n",
    "#var='cTot'\n",
    "\n",
    "#- initialize\n",
    "C_global_mat= np.empty([350,len(modellist),len(runlist),len(varlist)])\n",
    "C_highlat_mat= np.empty([350,len(modellist),len(runlist),len(varlist)])\n",
    "C_troplat_mat= np.empty([350,len(modellist),len(runlist),len(varlist)])\n",
    "C_midlat_mat= np.empty([350,len(modellist),len(runlist),len(varlist)])\n",
    "\n",
    "# create a time series of years for the first dimension\n",
    "ts= np.arange(350)\n",
    "\n",
    "for m in range(len(modellist)):\n",
    "#for m in range(len(['GFDL-ESM4',  'GISS_E2.1',  'NorESM2-LM','MPI-ESM1-2-LR'])):\n",
    "    model=modellist[m]\n",
    "    print('processing model: ' +model)\n",
    "\n",
    "    # get area and land fraction\n",
    "    ds_area = data_dict[modellist[m] +'_' +'areacella']\n",
    "    ds_landfrac = data_dict[modellist[m] +'_' +'landfrac']\n",
    "\n",
    "    #----loop over experiments----# \n",
    "    for e in range(len(runlist)):\n",
    "        run = runlist[e]\n",
    "        print('processing run: ' +run)\n",
    "\n",
    "        # get data from the dictionary\n",
    "        ds=data_dict[modellist[m] +'_' +runlist[e]]\n",
    "\n",
    "        # calculate land area\n",
    "        # adjust the area grid for CESM\n",
    "        if model=='CESM2':\n",
    "            area = ds_area['areacella'].squeeze().reindex_like(ds, method='nearest',tolerance=0.05)\n",
    "        else:\n",
    "            area = ds_area['areacella'].reindex_like(ds, method='nearest',tolerance=0.05)\n",
    "        \n",
    "        landfrac=ds_landfrac['sftlf'].reindex_like(ds, method='nearest',tolerance=0.05)\n",
    "        \n",
    "        if landfrac.max(dim=['lat','lon'])>1: #test if landfrac is on a 0-100 or 0-1 scale\n",
    "            landfrac=landfrac/100\n",
    "            \n",
    "        landarea=area*landfrac\n",
    "    \n",
    "        \n",
    "        #----loop over variables----#\n",
    "        for v in range(len(varlist)):\n",
    "            var=varlist[v]\n",
    "            print('processing variable: ' +var)\n",
    "\n",
    "            # load the drift correction\n",
    "            if model=='NorESM2-LM':\n",
    "                if var=='cVeg':\n",
    "                    field = pickle.load(open('/glade/campaign/cgd/tss/people/aswann/flat10/NorESM2-LM/NorESM2-LM_2D_TOTVEGC_ann_drift.pkl','rb'))\n",
    "                    adj_matrix = xr.DataArray(np.squeeze(field), dims=['lat','lon'], coords={'latitude': ds.lat, 'longitude':ds.lon})##,unit={'g C m-2 yr-1'})\n",
    "                    ty=ds['time'].dt.year\n",
    "                    tyindx=ty-ty[0]+1\n",
    "                    adjustment = adj_matrix* tyindx*(1/1000) #this is the drift for each time point and each gridcell in kg C m-2 yr-1\n",
    "\n",
    "                    ds[var]=ds[var]-adjustment # remove the drift from the variable\n",
    "                    \n",
    "                elif var=='cSoil':\n",
    "                    field = pickle.load(open('/glade/campaign/cgd/tss/people/aswann/flat10/NorESM2-LM/NorESM2-LM_2D_TOTSOMC_ann_drift.pkl','rb'))\n",
    "                    adj_matrix = xr.DataArray(np.squeeze(field), dims=['lat','lon'], coords={'latitude': ds.lat, 'longitude':ds.lon})##,unit={'g C m-2 yr-1'})\n",
    "                    ty=ds['time'].dt.year\n",
    "                    tyindx=ty-ty[0]+1\n",
    "                    adjustment = adj_matrix* tyindx*(1/1000) #this is the drift for each time point and each gridcell in kg C m-2 yr-1\n",
    "\n",
    "                    ds[var]=ds[var]-adjustment # remove the drift from the variable\n",
    "                    \n",
    "                elif var=='cLitter':\n",
    "                    field = pickle.load(open('/glade/campaign/cgd/tss/people/aswann/flat10/NorESM2-LM/NorESM2-LM_2D_TOTLITC_ann_drift.pkl','rb'))\n",
    "                    adj_matrix = xr.DataArray(np.squeeze(field), dims=['lat','lon'], coords={'latitude': ds.lat, 'longitude':ds.lon})##,unit={'g C m-2 yr-1'})\n",
    "                    ty=ds['time'].dt.year\n",
    "                    tyindx=ty-ty[0]+1\n",
    "                    adjustment = adj_matrix* tyindx*(1/1000) #this is the drift for each time point and each gridcell in kg C m-2 yr-1\n",
    "\n",
    "                    ds[var]=ds[var]-adjustment # remove the drift from the variable\n",
    "\n",
    "            \n",
    "            data_var= weighted_temporal_mean(ds, var) # this is probably memory intesive?\n",
    "\n",
    "            if var=='tas' or var=='pr': \n",
    "                # if this is *not* a carbon variable then we want to make an average\n",
    "                C_global =(((data_var*landarea)).sum(dim=['lat','lon']))/landarea.sum(dim=['lat','lon'])\n",
    "                C_highlat=(((data_var*landarea).where(ds.lat>=highlat)).sum(dim=['lat','lon']))/((landarea.where(ds.lat>=highlat)).sum(dim=['lat','lon']))\n",
    "                C_troplat=(((data_var*landarea).where((ds.lat>=-troplat) & (ds.lat<=troplat))).sum(dim=['lat','lon']))/((landarea.where((ds.lat>=-troplat) & (ds.lat<=troplat))).sum(dim=['lat','lon']))\n",
    "                C_midlat=(((data_var*landarea).where((ds.lat>=troplat) & (ds.lat<=highlat))).sum(dim=['lat','lon']))/((landarea.where((ds.lat>=troplat) & (ds.lat<=highlat))).sum(dim=['lat','lon']))\n",
    "    \n",
    "                #put into matrix \n",
    "                C_global_mat[0:len(C_global),m,e,v]= C_global\n",
    "                C_highlat_mat[0:len(C_global),m,e,v]= C_highlat\n",
    "                C_troplat_mat[0:len(C_global),m,e,v]= C_troplat\n",
    "                C_midlat_mat[0:len(C_global),m,e,v]= C_midlat\n",
    "            \n",
    "            else: # it is a carbon variable and we want to make a sum\n",
    "                # total carbon on land. Becuase it is in units of carbon/area (kgC/m2), multiply by area\n",
    "                # our area variable is in m2\n",
    "                C_global =(((data_var*landarea)).sum(dim=['lat','lon']))\n",
    "                C_highlat=((data_var*landarea).where(ds.lat>=highlat)).sum(dim=['lat','lon'])\n",
    "                C_troplat=((data_var*landarea).where((ds.lat>=-troplat) & (ds.lat<=troplat))).sum(dim=['lat','lon'])\n",
    "                C_midlat=((data_var*landarea).where((ds.lat>=troplat) & (ds.lat<=highlat))).sum(dim=['lat','lon'])\n",
    "    \n",
    "                #put into matrix and convert to PgC (kgC => PgC, divide by 10^12)\n",
    "                C_global_mat[0:len(C_global),m,e,v]= C_global*PgperKg\n",
    "                C_highlat_mat[0:len(C_global),m,e,v]= C_highlat*PgperKg\n",
    "                C_troplat_mat[0:len(C_global),m,e,v]= C_troplat*PgperKg\n",
    "                C_midlat_mat[0:len(C_global),m,e,v]= C_midlat*PgperKg\n",
    "\n",
    "            # reset values after the end of the time series to nan\n",
    "            C_global_mat[(len(C_global)):,m,e,v]=np.nan\n",
    "            C_highlat_mat[(len(C_highlat)):,m,e,v]=np.nan\n",
    "            C_troplat_mat[(len(C_troplat)):,m,e,v]=np.nan\n",
    "            C_midlat_mat[(len(C_midlat)):,m,e,v]=np.nan\n",
    "\n",
    "\n",
    "## for per area variables need area weighting\n",
    "#cTot_global = (((ds[var]*landarea)).sum(dim=['lat','lon']))/(landarea.sum(dim=['lat','lon']))\n",
    "#cTot_highlat=((ds[var]*landarea).where(ds.lat>=highlat)).sum(dim=['lat','lon'])/(landarea.where(ds.lat>=highlat).sum(dim=['lat','lon']))\n",
    "#cTot_troplat=((ds[var]*landarea).where((ds.lat>=-troplat) & (ds.lat<=troplat))).sum(dim=['lat','lon'])/(landarea.where((ds.lat>=-troplat) & (ds.lat<=troplat)).sum(dim=['lat','lon']))\n",
    "#cTot_midlat=((ds[var]*landarea).where((ds.lat>=troplat) & (ds.lat<=highlat))).sum(dim=['lat','lon'])/(landarea.where((ds.lat>=troplat) & (ds.lat<=highlat)).sum(dim=['lat','lon']))\n",
    "\n",
    "\n",
    " #----merge all variables into one dataset----#\n",
    "# if it's the first variable, then start a new datset, otherwise merge with existing\n",
    "#if v ==0:\n",
    "#    ds_cTot_global = cTot_global.copy()\n",
    "#else:\n",
    "#    #dsmerge_v=xr.merge([dsmerge_v, dsmerge_f])\n",
    "#    ds_cTot_global = xr.merge([ds_cTot_global, cTot_global])\n",
    "\n",
    "#dsmerge_v=xr.merge([dsmerge_v, dsmerge_f])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec86f798-b13d-4248-aede-063d0f1b88bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aecbf9a2-57f0-4e76-8430-dea01d4a0498",
   "metadata": {},
   "outputs": [],
   "source": [
    "# put the matrix into an xarray dataset\n",
    "latlist=['global','highlat','troplat','midlat']\n",
    "\n",
    "data_array_combined = np.full((len(ts), len(modellist), len(runlist), len(varlist), len(latlist)),np.nan)\n",
    "\n",
    "data_array_combined[:,:,:,:,0]=C_global_mat\n",
    "data_array_combined[:,:,:,:,1]=C_highlat_mat\n",
    "data_array_combined[:,:,:,:,2]=C_troplat_mat\n",
    "data_array_combined[:,:,:,:,3]=C_midlat_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2270d6f7-588a-4a58-a92c-f35a11afd41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(data_array_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b52bee7a-4c98-44c6-aa26-c8620ca215e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a09a6d-6dfb-4250-9e25-864428ed7f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "arg=C_global_mat[:,m,e,v]\n",
    "\n",
    "arg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129c80c7-b1b0-400f-a329-a5c1c25eaa3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "ds_C_global= xr.Dataset(\n",
    "    {\n",
    "        \"data\": ([\"time\", \"model\", \"run\", \"var\",\"latrange\"], data_array_combined)\n",
    "    },\n",
    "    coords={\n",
    "        \"time\": ts,\n",
    "        \"model\": modellist,\n",
    "        \"run\": runlist,\n",
    "        \"var\": varlist,\n",
    "        \"latrange\": latlist\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee4e842-df88-4715-abfc-c866c947f87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_C_global"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba55086-c525-4752-8835-002bacd86636",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - save the matrix to a netcdf file\n",
    "\n",
    "##ds_C_global.to_netcdf(\"C_metrics_matrix.nc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a8489d-d1f8-434d-8fde-555e34267817",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to load data from this ds\n",
    "\n",
    "# Access data for one model, run, var, latrange\n",
    "\n",
    "subset = ds_C_global.sel(model=modellist[0], run=runlist[0], var=varlist[0], latrange='global')\n",
    "\n",
    "print(subset)\n",
    "\n",
    "subset.data.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ad65a3-16cb-4e71-86a3-a54450cd3b09",
   "metadata": {},
   "source": [
    "## Shut down dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9b238c-5582-4884-8207-18b45335be72",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Once done, shut down the cluster\n",
    "# client.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773386d8-853c-44ff-b881-6f19b0d4e203",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # clean up\n",
    "# !rm ./dask-worker.e*\n",
    "# !rm ./dask-worker.o*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd96b21-fef5-47d0-ab8e-1d766786a0df",
   "metadata": {},
   "source": [
    "# #---------# Snippets below #-----------#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ff932e-e2d5-4be5-aae3-b5ee97570310",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e115acb-058b-410e-b52c-5ea999a644f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "480f892d-191b-425b-bf2b-b03a04706f88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78455144-77fb-476b-a563-d9a348a0215e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ccc17a-2ce5-425a-8390-b3bfc0561da6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7466c006-2ca7-455d-8fa0-af5fdf9ea729",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27421d3f-eeed-43b1-b493-d3119fb99ca2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NPL 2025b",
   "language": "python",
   "name": "npl-2025b"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
