{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ad12bdc-66e8-49ae-be6f-333a4cac80b1",
   "metadata": {},
   "source": [
    "# check output files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a99e78d7-570e-4e3b-b90f-4b9c36561646",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.matlib\n",
    "import numpy.ma as ma\n",
    "\n",
    "import xarray as xr\n",
    "#xr.set_options(enable_cftimeindex=True)\n",
    "#from xarray.coding.times import CFTimedeltaCoder\n",
    "time_coder = xr.coders.CFDatetimeCoder(use_cftime=True) #create time coder with cftime\n",
    "\n",
    "import time\n",
    "import cftime\n",
    "import netCDF4 as nc\n",
    "from datetime import timedelta\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "#import xcdat\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#import cartopy.crs as ccrs\n",
    "#from cartopy.util import add_cyclic_point\n",
    "\n",
    "import glob\n",
    "\n",
    "\n",
    "## notes on packages to add to this kernel\n",
    "import nc_time_axis\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c49cce4d-50b3-4e49-90fc-a563033334d8",
   "metadata": {},
   "source": [
    " float m01s19i013(pseudo_level, latitude, longitude) ;    =  Surface type fractions\n",
    "\n",
    "                int latitude_longitude ;\n",
    "                int pseudo_level(pseudo_level) ;\n",
    "                float latitude(latitude) ;\n",
    "                float longitude(longitude) ;\n",
    "                double forecast_period ;\n",
    "                double forecast_period_bnds(bnds) ;\n",
    "                double forecast_reference_time ;\n",
    "                double time ;\n",
    "                double time_bnds(bnds) ;\n",
    "                float m01s19i042(latitude, longitude) ;    LANDUSE CO2 FLUX TO ATM    KGC/M2/YR\n",
    "                float m01s19i044(latitude, longitude) ;    HARVEST C (GBM) KGC/M2/360d\n",
    "                float m01s19i053(latitude, longitude) ;    SOIL TO ATMOS RESP FLUX  KGC/M2/YR\n",
    "                float m01s19i102(latitude, longitude) ;    NPP POST N-LIMIT (GBM) KGC/M2/YR\n",
    "                float m01s30i464(latitude_0, longitude_0) ;   ATMOSPHERIC CO2 BURDEN IN GtC\n",
    "                float latitude_0(latitude_0) ;\n",
    "                float longitude_0(longitude_0) ;\n",
    "                float m01s30i466(latitude_0, longitude_0) ;    CO2 CONCENTRATION (ppmv)\n",
    "                float m01s00i250(latitude, longitude) ;       CO2 OCEAN FLUX             KG/M**2/S\n",
    "                float air_temperature(latitude, longitude) ;\n",
    "                double height ;\n",
    "                float air_temperature_0(latitude, longitude) ;\n",
    "                float air_temperature_1(dim0, latitude, longitude) ;\n",
    "                double forecast_period_0(dim0) ;\n",
    "                double forecast_period_0_bnds(dim0, bnds) ;\n",
    "                double time_0(dim0) ;\n",
    "                double time_0_bnds(dim0, bnds) ;\n",
    "                float soil_carbon_content(latitude, longitude) ;\n",
    "                float toa_outgoing_longwave_flux(latitude, longitude) ;\n",
    "                float toa_outgoing_longwave_flux_0(dim0, latitude, longitude) ;\n",
    "                float toa_outgoing_shortwave_flux(latitude, longitude) ;\n",
    "                float toa_outgoing_shortwave_flux_0(dim0, latitude, longitude) ;\n",
    "                float vegetation_carbon_content(latitude, longitude) ;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6645e88f-7634-44b2-bd91-7330218ffdb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputdir= '/glade/campaign/cgd/tss/people/aswann/flat10/'\n",
    "filenamepath = outputdir +'/testing/' +'dh493a.py21711201.nc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5982580d-02b4-4dee-a93b-d5cb9c678f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_r = dsmerge_f = xr.open_dataset(filenamepath, use_cftime=True, mask_and_scale=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f39a95a3-ed56-470f-a1aa-157aaeb782c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ds_masked = ds_r.where(ds_r >1e35)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5095b6ea-3e69-4d07-b06b-f469285d496a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce15179-1397-4893-a119-ae2f50102739",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_r.soil_carbon_content.where(ds_r.soil_carbon_content <1e35) .plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "503d5d5c-a4da-4b09-8bb0-6744e9d1edfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_r.vegetation_carbon_content.where(ds_r.soil_carbon_content <1e35) .plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b4c1a3-6778-4007-9380-2e0bc40f0921",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputdir= '/glade/campaign/cgd/tss/people/aswann/flat10/'\n",
    "\n",
    "model='ACCESS-ESM1-5'\n",
    "run='flat10_cdr'\n",
    "var='cVeg'\n",
    "\n",
    "searchpath= outputdir +model +'/' +run +'/*' +var +'*.nc'\n",
    "filenamelist= np.sort(glob.glob(searchpath))\n",
    "\n",
    "\n",
    "print(filenamelist)\n",
    "\n",
    "#ds_access=\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a14810-b022-439a-b771-29814d1f3c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19700307-6ecc-4fa5-a3a7-a32f091ee05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load custom functions for analyzing flat10\n",
    "\n",
    "from loading_function_flat10 import load_flat10, load_grid, select_time_slice, weighted_temporal_mean "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b00c56-7823-4504-ae59-28bebb6090d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e1db01-7d0c-4e85-a788-d31ffe0e8d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "modellist_t= ['ACCESS-ESM1-5',  \n",
    "            'CESM2',    \n",
    "            'GFDL-ESM4',  \n",
    "            'GISS_E2.1',  \n",
    "            'NorESM2-LM',\n",
    "            'MPI-ESM1-2-LR',\n",
    "            'CNRM-ESM2-1',\n",
    "            'HadCM3LC-Bris']\n",
    "\n",
    "runlist = ['flat10','flat10_zec','flat10_cdr']\n",
    "# use a wildcard to capture different ways the folders and runs are named across models\n",
    "runlist_wc = ['*lat10','*zec','*cdr']\n",
    "\n",
    "varlist=['tas']\n",
    "\n",
    "data_dict=load_flat10(data_dict, modellist_t, runlist, runlist_wc, varlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50eaea31-2528-4925-b272-75c9d9efa94d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545c3cf0-5e37-4571-b9c2-e2eb5fb4e546",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d129e978-ef35-459c-ad96-2851a83b3ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputdir= '/glade/campaign/cgd/tss/people/aswann/flat10/'\n",
    "model='NorESM2-LM'\n",
    "run='flat10_zec'\n",
    "var='cSoil'\n",
    "\n",
    "# single file\n",
    "#filename='cSoil_Lmon_NorESM2-LM_flat10-zec_r1i1p1f1_gn_185002-185912.nc'\n",
    "\n",
    "#outputdir +model +'/' +run +'/*' +var +'_*.nc'\n",
    "\n",
    "#searchpath = outputdir +model +'/' +run +'/cSoil*.nc'\n",
    "searchpath= outputdir +model +'/' +run +'/*' +var +'_*.nc'\n",
    "\n",
    "filenamelist= np.sort(glob.glob(searchpath))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d726cbd-afcf-44f3-aab1-fe921acfa8ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "filenamelist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f13c9c6-4432-4c21-b596-5d9cdc0b9c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds=xr.open_dataset('/glade/campaign/cgd/tss/people/aswann/flat10/NorESM2-LM/flat10_zec/cSoil_Emon_NorESM2-LM_flat10_zec_r1i1p1f1_gn_196001-196912.nc', decode_times=time_coder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7d0fcc-e416-4b71-9cbb-1710713ecb9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds['cSoil'].mean(dim='lon').plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b553c43-9da5-494d-a1fd-4fe330945667",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load custom functions for analyzing flat10\n",
    "\n",
    "from loading_function_flat10 import load_flat10, load_grid, select_time_slice, weighted_temporal_mean \n",
    "\n",
    "modellist=['NorESM2-LM']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c83367-6ad7-4a6c-81e6-a35c40b8bb3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load grid\n",
    "data_dict = load_grid(data_dict,modellist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e6d36c-a9bd-4471-bdf6-dc2e829b74dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_area = data_dict[model +'_' +'areacella']\n",
    "ds_landfrac = data_dict[model +'_' +'landfrac']\n",
    "#area = ds_area['areacella'].expand_dims(dim={'time': ds.time.size}, axis=0)\n",
    "#landfrac=ds_landfrac['sftlf'].expand_dims(dim={'time': ds.time.size}, axis=0)\n",
    "\n",
    "area = ds_area['areacella'].reindex_like(ds, method='nearest',tolerance=0.05)\n",
    "\n",
    "landfrac=ds_landfrac['sftlf'].reindex_like(ds, method='nearest',tolerance=0.05)\n",
    "\n",
    "if landfrac.max(dim=['lat','lon'])>1: #test if landfrac is on a 0-100 or 0-1 scale\n",
    "    landfrac=landfrac/100\n",
    "    \n",
    "landarea=area*landfrac\n",
    "\n",
    "\n",
    "landarea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f742d4c2-052c-4e0c-bf5c-22e26be6abd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "landarea.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efaa81a3-985f-4310-b5d0-db744baeb0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "csoil=ds.cSoil\n",
    "csoil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c14801-9a20-4820-82a8-6430c29109b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "csoil.sum(dim=['lat','lon']).plot()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "614e2c69-1915-4103-932e-b2d1acdc5d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "C_global =(csoil*landarea).sum(dim=['lat','lon'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a912dd7b-a7e4-4ca3-bb61-75a2ca40344c",
   "metadata": {},
   "outputs": [],
   "source": [
    "C_global.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca3430e-6e09-4108-91f5-528d2e350992",
   "metadata": {},
   "source": [
    "# Zonal correction for NorESM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118b5ef2-8f9c-4dd1-99b0-671944cf95b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load custom functions for analyzing flat10\n",
    "\n",
    "from loading_function_flat10 import load_flat10, load_grid, select_time_slice, weighted_temporal_mean "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9550009e-66d5-4f87-ae72-07ee3dc64030",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputdir= '/glade/campaign/cgd/tss/people/aswann/flat10/'\n",
    "\n",
    "modellist_orig= ['ACCESS-ESM1-5',  \n",
    "            'CESM2',    \n",
    "            'GFDL-ESM4',  \n",
    "            'GISS_E2.1',  \n",
    "            'NorESM2-LM',\n",
    "            'MPI-ESM1-2-LR',\n",
    "            'CNRM-ESM2-1',\n",
    "            'HadCM3LC-Bris']\n",
    "\n",
    "\n",
    "runlist = ['flat10','flat10_zec','flat10_cdr']\n",
    "# use a wildcard to capture different ways the folders and runs are named across models\n",
    "runlist_wc = ['*lat10','*zec','*cdr']\n",
    "\n",
    "varlist_load=['cVeg','cSoil','cLitter','nbp','gpp','rh'] #, 'gpp','fgco2', 'ra', 'rh']#, 'npp'] # not working beyond nbp for norESM\n",
    "varlist_analyze=['cVeg','cSoil','cTot','cLitter','nbp','gpp','rh']\n",
    "varlist=varlist_load\n",
    "unitslist=['kgC m-2','kgC m-2','kgC m-2','kgC m-2 s-1','kgC m-2 s-1','kgC m-2 s-1']\n",
    "\n",
    "# there seems to be a problem with ra for NorESM\n",
    "\n",
    "modelcolors=['tab:blue','tab:orange','tab:green','tab:red','tab:gray','tab:purple','tab:cyan','gold','tab:brown']\n",
    "### from ben: colors=[\"tab:cyan\",\"tab:olive\",\"tab:green\",\"tab:red\",\"tab:gray\",\"tab:pink\",\"limegreen\",\"tab:brown\", \"slateblue\",\"gold\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd7144c-f408-4df8-9f9e-e92e148c3f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "modellist=['NorESM2-LM']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "735a8560-f200-40e1-a4d8-cef0a17b4a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize a dictionary to hold all of the data\n",
    "data_dict={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d56b19-825f-48cf-81b8-4fa6aa8f006f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load grid\n",
    "data_dict = load_grid(data_dict,modellist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d47899-f8bf-4d5e-bad2-b1f705117441",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all data\n",
    "data_dict=load_flat10(data_dict, modellist, runlist, runlist_wc, varlist)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c091a3f-b153-4be3-89c2-e60c325a3c58",
   "metadata": {},
   "source": [
    "### First lets try the 2d fields from Jerry Tjiputra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8167c99-b8ec-4445-aca8-56c56506310c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59402b0c-9485-43ab-8a74-10303e956da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## THESE BOTH REQUIRE A RECENT VERSION OF NUMPY.... \n",
    "# must use NPL 2025b (not a 2023 version)\n",
    "\n",
    "totveg_corr=pd.read_pickle('NorESM2-LM_2D_TOTVEGC_ann_drift.pkl') \n",
    "\n",
    "field = pickle.load(open('NorESM2-LM_2D_TOTVEGC_ann_drift.pkl','rb'))\n",
    "\n",
    "# units are  [g C m-2 yr-1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c608fff2-f576-4ae2-8746-517a0c8f7de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "totveg_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5940c55-f687-4fc8-8a3a-8960f64d3d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a49511-00ac-40fc-8e0a-a399e9dca2ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(np.squeeze(field))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e4e5a14-c3b4-4bc0-a9ab-a424663d7496",
   "metadata": {},
   "outputs": [],
   "source": [
    "m=0\n",
    "\n",
    "ds0 = data_dict[modellist[m] +'_' +runlist[0]] #flat10\n",
    "ds1 = data_dict[modellist[m] +'_' +runlist[1]] #flat10-zec\n",
    "ds2 = data_dict[modellist[m] +'_' +runlist[2]] #flat10-cdr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2493ae97-d8db-4ad8-ba47-1c40fc3527c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make xarray dataarrays to make it easier to apply to time dimensions\n",
    "\n",
    "# for flat10, starts at 1850\n",
    "adj_matrix = xr.DataArray(np.squeeze(field), dims=['lat','lon'], coords={'latitude': ds0.lat, 'longitude':ds0.lon})##,unit={'g C m-2 yr-1'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26adfee4-09c9-455d-82f3-9e9392239175",
   "metadata": {},
   "outputs": [],
   "source": [
    "ty=ds0['time'].dt.year\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecdac473-f654-4285-9e28-5e26894ca77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e492d0c-2ce5-4fce-9331-53c915291a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tyindx=ty-ty[0]+1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6272ee-9073-4f86-922d-118ccbc708b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tyindx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b3825c-f143-42ab-8221-63a1107aa54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ds0_adj = adj_matrix* tyindx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9df6847-8ced-4758-80ce-658b6823ca4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds0_adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb20d62d-7540-4467-aad6-d9ceff98251a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make xarray dataarrays to make it easier to apply to time dimensions\n",
    "\n",
    "# for flat10, starts at 1850\n",
    "adj_matrix = xr.DataArray(np.squeeze(field), dims=['lat','lon'], coords={'lat': ds0.lat, 'lon':ds0.lon})##,unit={'g C m-2 yr-1'})\n",
    "##time_vector = xr.DataArray(np.arange(len(ds0.time)), dims='time', coords={'time': ds0.time})\n",
    "ty=ds0['time'].dt.year\n",
    "tyindx=ty-ty[0]+1\n",
    "#time_vector = xr.DataArray(tyindx, dims='time', coords={'time': ds0.time})\n",
    "\n",
    "ds0_adj = adj_matrix* tyindx\n",
    "\n",
    "\n",
    "ds0_adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6da8d12-e4c9-4cdf-b276-e1ad94008419",
   "metadata": {},
   "outputs": [],
   "source": [
    "#arg=ds0.time.year\n",
    "arg=ds0['time'].dt.year\n",
    "\n",
    "arg-arg[0]+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ebfd399-b2df-4baa-b975-4de26e686ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds0_adj.mean(dim='lon').plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3c863f-2beb-4a3a-9e0d-6a47433642da",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds1.mean(dim='lon').plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e07c97-b01f-4f4a-aba2-cb52ca7f7a14",
   "metadata": {},
   "source": [
    "### Zonal files"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2d06303e-e9a6-47dc-9da4-4a9dd421a6e4",
   "metadata": {},
   "source": [
    "### For NorESM, info from Jerry Tjiputra on correcting drift\n",
    "\n",
    "Abby, attached is the drift for NorESM2-LM veg, soil, and litt C content calculated from zonally averaged values over 300 yrs esm-piCtrl (NorESM2-LM_zonally_drift_Veg_Soil_Litt_C_1850-2199.dat), which can be read with python:\n",
    "\n",
    "===========================================================================  \n",
    "field = pickle.load(open('NorESM2-LM_zonally_drift_Veg_Soil_Litt_C_1850-2199.dat', 'rb'))   \n",
    "lat   = field[0] #latitude  \n",
    "tr_veg = field[1] #trend in cumulative vegetation C [PgC/yr]  \n",
    "tr_soi = field[2] #trend in cumulative soil C [PgC/yr]  \n",
    "tr_lit = field[3] #trend in cumulative litter C [PgC/yr]  \n",
    "tr_veg_avg = field[4] #trend in averaged vegetation C [gC/m2/yr]  \n",
    "tr_soi_avg = field[5] #trend in averaged soil C [gC/m2/yr]  \n",
    "tr_lit_avg = field[6] #trend in averaged litter C [gC/m2/yr]  \n",
    "===========================================================================  \n",
    "\n",
    "How to apply:\n",
    "=======for flat10=============\n",
    "Yr_start = 1850\n",
    "for yr in np.arange(yr_start, yr_end)\n",
    "  soilC[yr-yr_start, lat] = soilC[yr-yr_start, lat] - drift_soilC[lat]*(yr-1850)\n",
    "\n",
    "=======for flat10-cdr=============\n",
    "Yr_start = 1950\n",
    "for yr in np.arange(yr_start, yr_end)\n",
    "  soilC[yr-yr_start, lat] = soilC[yr-yr_start, lat] - drift_soilC[lat]*(yr-1850)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a265f1-18f9-48fc-be82-459d1b091bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1845a59b-c09f-4895-8f6e-d45ee90c85d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load zonal corrections for NorESM\n",
    "\n",
    "field = pickle.load(open('NorESM2-LM_zonally_drift_Veg_Soil_Litt_C_1850-2199.dat', 'rb'))   \n",
    "lat   = field[0] #latitude  \n",
    "tr_veg = field[1] #trend in cumulative vegetation C [PgC/yr]  \n",
    "tr_soi = field[2] #trend in cumulative soil C [PgC/yr]  \n",
    "tr_lit = field[3] #trend in cumulative litter C [PgC/yr]  \n",
    "tr_veg_avg = field[4] #trend in averaged vegetation C [gC/m2/yr]  \n",
    "tr_soi_avg = field[5] #trend in averaged soil C [gC/m2/yr]  \n",
    "tr_lit_avg = field[6] #trend in averaged litter C [gC/m2/yr]  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ece0757-7a03-47b8-b8fe-d8fe65c594ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "m=0\n",
    "\n",
    "ds0 = data_dict[modellist[m] +'_' +runlist[0]] #flat10\n",
    "ds1 = data_dict[modellist[m] +'_' +runlist[1]] #flat10-zec\n",
    "ds2 = data_dict[modellist[m] +'_' +runlist[2]] #flat10-cdr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1581abfd-73e5-4979-9c42-63b8fb4dcc22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make xarray vectors to make it easier to apply to time dimensions\n",
    "\n",
    "# for flat10, starts at 1850\n",
    "adj_vector = xr.DataArray(tr_veg, dims='lat', coords={'latitude': ds0.lat})\n",
    "time_vector = xr.DataArray(np.arange(len(ds0.time)), dims='time', coords={'time': ds0.time})\n",
    "\n",
    "ds0_adj = adj_vector * time_vector\n",
    "\n",
    "ds0_adj.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e8b5fd-49e0-40ae-8524-1b31e4e996b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the adjustment needs to be different for the other experiments since they start later in time\n",
    "\n",
    "# for flat10-zec, starts at 1950\n",
    "adj_vector = xr.DataArray(tr_veg, dims='lat', coords={'latitude': ds0.lat})\n",
    "time_vector = xr.DataArray(np.arange(len(ds1.time)), dims='time', coords={'time': ds1.time})\n",
    "\n",
    "start_year = 1850\n",
    "\n",
    "years_since_start = xr.DataArray(\n",
    "    ds1['time'].dt.year - start_year,\n",
    "    coords={'time': ds1['time']},\n",
    "    dims='time'\n",
    ")\n",
    "\n",
    "\n",
    "# Broadcast to match the shape of data (time, lat)\n",
    "adjustment = years_since_start * adj_vector\n",
    "\n",
    "\n",
    "# Remove the trend\n",
    "ds1_adj = ds1.cVeg.mean(dim='lon') - adjustment\n",
    "\n",
    "\n",
    "adjustment.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64f08e9-fb09-4a19-bf95-88cce59bc477",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "ds1_adj.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf86db4-89b7-4f30-ab33-f28cc0de8f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adj_NorESM_zonal(dsz):\n",
    "\n",
    "    \"\"\"\n",
    "    adjusts the zonal mean profile in dsz by trend_zonal\n",
    "    specific for files sent by NorESM\n",
    "    \"\"\"\n",
    "    # for flat10-zec, time series starts at 1950\n",
    "    time_vector = xr.DataArray(np.arange(len(dsz.time)), dims='time', coords={'time': dsz.time})\n",
    "    \n",
    "    start_year = 1850\n",
    "    \n",
    "    years_since_start = xr.DataArray(\n",
    "        dsz['time'].dt.year - start_year,\n",
    "        coords={'time': dsz['time']},\n",
    "        dims='time'\n",
    "    )\n",
    "\n",
    "    # load trends to apply, load zonal corrections for NorESM\n",
    "    import pickle\n",
    "    field = pickle.load(open('NorESM2-LM_zonally_drift_Veg_Soil_Litt_C_1850-2199.dat', 'rb'))   \n",
    "    lat   = field[0] #latitude  \n",
    "    tr_veg = field[1] #trend in cumulative vegetation C [PgC/yr]  \n",
    "    tr_soi = field[2] #trend in cumulative soil C [PgC/yr]  \n",
    "    tr_lit = field[3] #trend in cumulative litter C [PgC/yr]  \n",
    "    tr_veg_avg = field[4] #trend in averaged vegetation C [gC/m2/yr]  \n",
    "    tr_soi_avg = field[5] #trend in averaged soil C [gC/m2/yr]  \n",
    "    tr_lit_avg = field[6] #trend in averaged litter C [gC/m2/yr]  \n",
    "\n",
    "    adj_vector = xr.DataArray(trend_zonal, dims='lat', coords={'latitude': dsz.lat})\n",
    "    \n",
    "    # Broadcast to match the shape of data (time, lat)\n",
    "    adjustment = years_since_start * adj_vector\n",
    "\n",
    "    \n",
    "    # Remove the trend\n",
    "    dsz_adj[var] = dsz[var].mean(dim='lon') - adjustment # this needs to either be a new entry in dsz or a new dataset with all variables?\n",
    "\n",
    "    return dsz\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8736815-98e8-46f4-9bff-ba0311f93253",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds1_adj=adj_NorESM_zonal(dz1,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5018466b-aaac-4f28-8dcb-e47e530ce7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Inputs\n",
    "# Example trend: 0.02 units per year\n",
    "trend = 0.02  \n",
    "start_year = 2000  # Year to treat as baseline\n",
    "\n",
    "# Assume `data` is your xarray.DataArray with dims (\"time\", \"lat\")\n",
    "# Convert time to years since start_year\n",
    "years_since_start = xr.DataArray(\n",
    "    data['time'].dt.year - start_year,\n",
    "    coords={'time': data['time']},\n",
    "    dims='time'\n",
    ")\n",
    "\n",
    "# Expand years_since_start to match shape of data\n",
    "# So it can broadcast across the 'lat' dimension\n",
    "adjustment = trend * years_since_start\n",
    "adjustment = adjustment.broadcast_like(data)\n",
    "\n",
    "# Remove the trend\n",
    "detrended = data - adjustment\n",
    "\n",
    "\n",
    "###\n",
    "import xarray as xr\n",
    "\n",
    "# Inputs:\n",
    "# - data: DataArray with dims (\"time\", \"lat\")\n",
    "# - trend: DataArray with dims (\"lat\",), one trend per latitude\n",
    "# - start_year: e.g., 2000\n",
    "\n",
    "# Compute years since the start year\n",
    "years_since_start = xr.DataArray(\n",
    "    data['time'].dt.year - start_year,\n",
    "    coords={'time': data['time']},\n",
    "    dims='time'\n",
    ")\n",
    "\n",
    "# Broadcast to match the shape of data (time, lat)\n",
    "adjustment = years_since_start * trend\n",
    "\n",
    "# Remove the trend\n",
    "detrended = data - adjustment\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a4d95c-2e4c-42b1-a63f-a404cff315aa",
   "metadata": {},
   "source": [
    "# Loading UKESM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7454bd4e-e80e-4da7-a3b4-556a41dd9663",
   "metadata": {},
   "outputs": [],
   "source": [
    "modellist=['UKESM1.2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638f2e94-c809-43b2-98a0-9bd96dbd65d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#---- get grid info\n",
    "\n",
    "# loop over models\n",
    "for m in range(len(modellist)):\n",
    "    model=modellist[m]\n",
    "\n",
    "    print(model +' getting grid info')\n",
    "    # get land fraction\n",
    "    filenamelist= glob.glob(outputdir +model +'/*/*sftlf*.nc')\n",
    "    landfrac = xr.open_dataset(filenamelist[0], use_cftime=True)\n",
    "\n",
    "    # get area of gridcells\n",
    "    filenamelist= glob.glob(outputdir +model +'/*/*areacella*.nc')\n",
    "    areacella = xr.open_dataset(filenamelist[0], use_cftime=True)\n",
    "\n",
    "    ## get area of ocean gridcells\n",
    "    #filenamelist= glob.glob(outputdir +model +'/*/*areacello*.nc')\n",
    "    #areacello = xr.open_dataset(filenamelist[0], use_cftime=True)\n",
    "    #if model =='CESM2':\n",
    "    #    areacello=areacello*1e-4 # CESM2 has area units of cm2 for ocean\n",
    "\n",
    "    #----correct the name of the lat lon dimensions for landfrac and areacella\n",
    "    if ((model =='HadCM3LC-Bris' or 'UKESM1.2') and ('lat' not in landfrac)):\n",
    "        #-- change latitude and longitude to lat and lon for HadCM3\n",
    "        landfrac = landfrac.rename({'longitude': 'lon','latitude': 'lat'})\n",
    "        areacella = areacella.rename({'longitude': 'lon','latitude': 'lat'})\n",
    "        #-- change name of area fields to match other models\n",
    "        areacella = areacella.rename({'cell_area': 'areacella'})\n",
    "        landfrac = landfrac.rename({'land_area_fraction': 'sftlf'})\n",
    "\n",
    "    if (model =='GISS_E2.1'):\n",
    "        # lon is -180 to 180 in data but 0 to 360 in grid files =>convert\n",
    "        areacella['lon']=areacella['lon']-180\n",
    "        landfrac['lon']=landfrac['lon']-180\n",
    "        #landfrac['lon']=landfrac['lon']-180\n",
    "        #landfrac.reindex_like(areacella, method='nearest',tolerance=0.05)\n",
    "        \n",
    "    # add to the dictionary\n",
    "    data_dict[model +'_areacella'] = areacella\n",
    "    data_dict[model +'_landfrac'] = landfrac\n",
    "    #data_dict[model +'_areacello'] = areacello"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d20e213-add6-4fc8-9b0e-9f2bf8764ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#modellist = ['NorESM2-LM']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10f25b1-163d-4d6b-9951-f19b8968695d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this loads all variables in varlist for all experiments in runlist and models in modellist\n",
    "\n",
    "#modellist=['GFDL-ESM4',  'GISS_E2.1',  'NorESM2-LM','MPI-ESM1-2-LR']\n",
    "#modellist=['HadCM3LC-Bris']\n",
    "#----loop over models----#\n",
    "\n",
    "for m in range(len(modellist)):\n",
    "#for m in range(len(['GFDL-ESM4',  'GISS_E2.1',  'NorESM2-LM','MPI-ESM1-2-LR'])):\n",
    "    model=modellist[m]\n",
    "    print('loading model: ' +model)\n",
    "    #----loop over experiments----# \n",
    "    for r in range(len(runlist)):\n",
    "        run = runlist_wc[r]\n",
    "        print('loading run: ' +run)\n",
    "        #----loop over variables----#\n",
    "        for v in range(len(varlist)):\n",
    "            var=varlist_load[v]\n",
    "            print('loading variable: ' +var)\n",
    "            \n",
    "            searchpath= outputdir +model +'/' +run +'/*' +var +'_*.nc'\n",
    "            if ((model =='CESM2') or (model == 'UKESM1.2')):\n",
    "                # all models have a var_ filename except CESM\n",
    "                searchpath= outputdir +model +'/' +run +'/*' +var +'*.nc'\n",
    "            \n",
    "            filenamelist= np.sort(glob.glob(searchpath)) # sort in time order, xarray was having trouble arranging some of them in time dim\n",
    "\n",
    "            #----loop over filenames----#\n",
    "            # some variables are stored in multiple files\n",
    "            # this should be possible with xr.open_mfdataset but it isn't loading all of time points\n",
    "            for f in range(len(filenamelist)):\n",
    "                file = filenamelist[f]\n",
    "                if f==0:\n",
    "                    dsmerge_f = xr.open_dataset(file, use_cftime=True)\n",
    "                else:\n",
    "                    ds = xr.open_dataset(file, use_cftime=True)\n",
    "                    dsmerge_f=xr.concat([dsmerge_f,ds],dim='time')\n",
    "\n",
    "            #----- Dealing with GISS----#\n",
    "            # GISS does not have a \"time\" index, and instead just a list of years\n",
    "            # lets replace the \"year\" dimension (data is called \"years\")\n",
    "            # with a cftime object called \"time\" so it matches the other models\n",
    "            # some variables don't have the variable that defines years at all\n",
    "            if ((model == 'GISS_E2.1') and ('time' not in dsmerge_f)):         \n",
    "                if 'year' in dsmerge_f: # if it has a variable called year, use that to make the time index\n",
    "                    time_index = [cftime.DatetimeNoLeap(year, 1, 1) for year in dsmerge_f.year]\n",
    "                else: # if it does not have a variable for year, use the size of the year dimension to make the time index\n",
    "                    startyear=[1850, 1950, 1950] # these are the start years for each experiment for GISS\n",
    "                    years = np.arange(startyear[r], startyear[r]+len(dsmerge_f['year']))\n",
    "                    time_index = [cftime.DatetimeNoLeap(year, 1, 1) for year in years]\n",
    "                \n",
    "                # Create a new DataArray with cftime objects\n",
    "                time_da = xr.DataArray(time_index, dims='year')\n",
    "                # Add time_da as a coordinate to the dataset\n",
    "                dsmerge_f.coords['time'] = time_da\n",
    "                # Now, swap dimensions from 'years' to 'time'\n",
    "                dsmerge_f = dsmerge_f.swap_dims({'year': 'time'})\n",
    "                # drop the year variable\n",
    "                #dsmerge_f = dsmerge_f.drop_vars('year')\n",
    "            \n",
    "            #----correct the name of the lat lon dimensions\n",
    "            if ((model =='HadCM3LC-Bris') or (model == 'UKESM1.2') and ('lat' not in dsmerge_f)):\n",
    "                #-- change latitude and longitude to lat and lon for HadCM3\n",
    "                dsmerge_f = dsmerge_f.rename({'longitude': 'lon','latitude': 'lat'})\n",
    "            \n",
    "            #----correct variable names----# \n",
    "            if 'nep' in dsmerge_f: # one model has nbp called nep instead -> add an nbp variable that is a copy of nep\n",
    "                dsmerge_f['nbp'] = dsmerge_f['nep']\n",
    "                #dsmerge_f = dsmerge_f.drop_vars('nep') # to remove it from the dataset\n",
    "            \n",
    "            if model =='HadCM3LC-Bris':\n",
    "                if 'GBMVegCarb_srf' in dsmerge_f: #HadCM3 \n",
    "                    dsmerge_f['cVeg'] = dsmerge_f['GBMVegCarb_srf']\n",
    "                if 'soilCarbon_srf' in dsmerge_f: #HadCM3 \n",
    "                    dsmerge_f['cSoil'] = dsmerge_f['soilCarbon_srf']\n",
    "                if 'NPP_mm_srf' in dsmerge_f: #HadCM3 \n",
    "                    dsmerge_f['npp'] = dsmerge_f['NPP_mm_srf']\n",
    "                if 'unknown' in dsmerge_f: #HadCM3 \n",
    "                    dsmerge_f['nbp'] = dsmerge_f['unknown']\n",
    "                if 'field1560_mm_srf' in dsmerge_f: #HadCM3 \n",
    "                    dsmerge_f['fgco2'] = dsmerge_f['field1560_mm_srf']\n",
    "                if 'soilResp_mm_srf' in dsmerge_f: #HadCM3 cSoil\n",
    "                    dsmerge_f['rh'] = dsmerge_f['soilResp_mm_srf']\n",
    "                if 'GPP_mm_srf gpp' in dsmerge_f: #HadCM3 cSoil\n",
    "                    dsmerge_f['gpp'] = dsmerge_f['GPP_mm_srf gpp']\n",
    "            \n",
    "            if model =='UKESM1.2':\n",
    "                missing_value = 1.0e36\n",
    "                for var_name, vari in dsmerge_f.data_vars.items():\n",
    "                    # Apply only if variable is numeric and has at least one dimension\n",
    "                    if np.issubdtype(vari.dtype, np.number):\n",
    "                        dsmerge_f[var_name] = vari.where(vari < missing_value * 0.1, np.nan)\n",
    "                if 'vegetation_carbon_content' in dsmerge_f: #UKESM \n",
    "                    dsmerge_f['cVeg'] = dsmerge_f['vegetation_carbon_content']\n",
    "                if 'soil_carbon_content' in dsmerge_f: #UKESM\n",
    "                    dsmerge_f['cSoil'] = dsmerge_f['soil_carbon_content']\n",
    "                if 'm01s19i102' in dsmerge_f: #UKESM \n",
    "                    dsmerge_f['npp'] = dsmerge_f['m01s19i102']\n",
    "                #if 'unknown' in dsmerge_f: #UKESM  ###I CANT FIND NBP, will need to be constructed from soil resp, plant resp, and gpp? should verify with UKESM group\n",
    "                #    dsmerge_f['nbp'] = dsmerge_f['unknown']\n",
    "                if 'm01s00i250' in dsmerge_f: #UKESM\n",
    "                    dsmerge_f['fgco2'] = dsmerge_f['m01s00i250']\n",
    "                if 'm01s19i053' in dsmerge_f: #UKESM\n",
    "                    dsmerge_f['rh'] = dsmerge_f['m01s19i053']\n",
    "                if 'm01s19i183' in dsmerge_f: #UKESM   \n",
    "                    dsmerge_f['gpp'] = dsmerge_f['m01s19i183']\n",
    "\n",
    "\n",
    "            \n",
    "            #----check units and convert if necessary----#\n",
    "            if var in dsmerge_f: \n",
    "                if model =='CESM2':\n",
    "                    if dsmerge_f[var].units == 'gC/m^2/s':\n",
    "                        dsmerge_f[var]=dsmerge_f[var]*(1/1000) # convert from gC to kgC\n",
    "                        dsmerge_f[var].attrs['units'] = 'kg m-2 s-1'\n",
    "                    # stock variables\n",
    "                    elif dsmerge_f[var].units == 'gC/m^2':\n",
    "                        dsmerge_f[var]=dsmerge_f[var]*(1/1000) # convert from gC to kgC\n",
    "                        dsmerge_f[var].attrs['units'] = 'kg m-2'\n",
    "\n",
    "                # the units for cVeg in GISS look like they MUST be in gC rather than kgC \n",
    "                # CHANGING THE UNIT - even though it is reported as kgC, assuming it is in gC\n",
    "                if ((var == 'cVeg') and (model == 'GISS_E2.1')):\n",
    "                    dsmerge_f[var]=dsmerge_f[var]*(1/1000) # convert from gC to kgC\n",
    "\n",
    "                \n",
    "            else: #var does not exist\n",
    "                ds=dsmerge_f\n",
    "                # add a blank variable so that loops work\n",
    "                if 'time' in ds:\n",
    "                    nan_dataarray = xr.DataArray(np.full((len(ds['time']),len(ds['lat']), len(ds['lon'])), np.nan), \n",
    "                                                 coords={'lon': ds['lon'], 'lat': ds['lat'],'time': ds['time']}, dims=['time','lat', 'lon'])\n",
    "                #else: # this should now be obsolete\n",
    "                #    nan_dataarray = xr.DataArray(np.full((len(ds['year']),len(ds['lat']), len(ds['lon'])), np.nan), \n",
    "                #             coords={'lon': ds['lon'], 'lat': ds['lat'],'year': ds['year']}, dims=['year','lat', 'lon'])\n",
    " \n",
    "   \n",
    "                # Assign the new variable to the dataset\n",
    "                dsmerge_f[var] = nan_dataarray\n",
    "            \n",
    "            #----merge all variables into one dataset----#\n",
    "            # if it's the first variable, then start a new datset, otherwise merge with existing\n",
    "            if v ==0:\n",
    "                dsmerge_v = dsmerge_f.copy()\n",
    "            else:\n",
    "                dsmerge_v=xr.merge([dsmerge_v, dsmerge_f])\n",
    "\n",
    "            # add a new variable that is the sum of all carbon pools\n",
    "            if all(var_name in dsmerge_v for var_name in ['cVeg', 'cSoil', 'cLitter']):\n",
    "                if (dsmerge_v['cLitter'].notnull().all()): #litter is sometimes missing. Would be good to make this more general but dealing with this problem for now.\n",
    "                    dsmerge_v['cTot'] = dsmerge_v['cVeg']+dsmerge_v['cSoil']+dsmerge_v['cLitter'] \n",
    "                else: \n",
    "                    dsmerge_v['cTot'] = dsmerge_v['cVeg']+dsmerge_v['cSoil'] \n",
    "                \n",
    "        #----save output to a dictionary----#\n",
    "        print('adding ' +model +' ' +runlist[r] +' to dict')\n",
    "        data_dict[model +'_' +runlist[r]] = dsmerge_v\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1627994-3f55-4b3e-8817-8ec69789b966",
   "metadata": {},
   "outputs": [],
   "source": [
    "arg=dsmerge_v.soil_carbon_content.mean(dim='time').values\n",
    "arg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3534ba5c-a16c-4a9c-b454-ab1284f2af43",
   "metadata": {},
   "outputs": [],
   "source": [
    "(dsmerge_f.time.values-dsmerge_v.time.values).plot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5796e045-1e5b-400e-b091-1bf407d183f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "testnc=xr.open_dataset('/glade/campaign/cgd/tss/people/aswann/flat10/UKESM1.2/flat10/test.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "558bc4e5-d40e-4c04-bbaa-9148504f0d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "arg=testnc.soil_carbon_content.mean(dim='time').values\n",
    "arg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7faab0-fb90-45e5-af36-b81d2ea9dc35",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_value = 1.0e36\n",
    "for var in ds.data_vars:\n",
    "    ds[var] = ds[var].where(ds[var] < missing_value * 0.1, np.nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc9cbc42-7611-457b-bae7-3abd1f82458e",
   "metadata": {},
   "source": [
    "# Loading CO2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbac7b4b-df66-477d-a8b2-ea41f382b12c",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputdir= '/glade/campaign/cgd/tss/people/aswann/flat10/'\n",
    "\n",
    "modellist_orig= ['ACCESS-ESM1-5',  \n",
    "            'CESM2',    \n",
    "            'GFDL-ESM4',  \n",
    "            'GISS_E2.1',  \n",
    "            'NorESM2-LM',\n",
    "            'MPI-ESM1-2-LR',\n",
    "            'CNRM-ESM2-1',\n",
    "            'HadCM3LC-Bris',\n",
    "            'UKESM1.2']\n",
    "modellist=modellist_orig\n",
    "\n",
    "runlist = ['flat10','flat10_zec','flat10_cdr']\n",
    "# use a wildcard to capture different ways the folders and runs are named across models\n",
    "runlist_wc = ['*lat10','*zec','*cdr']\n",
    "\n",
    "varlist_load=['cVeg','cSoil','cLitter','nbp','gpp','rh','tas','pr'] #, 'gpp','fgco2', 'ra', 'rh']#, 'npp'] # not working beyond nbp for norESM\n",
    "varlist_analyze=['cVeg','cSoil','cTot','cLitter','nbp','gpp','rh','tas','pr']\n",
    "varlist=varlist_load\n",
    "unitslist=['kgC m-2','kgC m-2','kgC m-2','kgC m-2 s-1','kgC m-2 s-1','kgC m-2 s-1','K','kg m-2 s-1']\n",
    "\n",
    "# there seems to be a problem with ra for NorESM\n",
    "\n",
    "modelcolors=['tab:blue','tab:orange','tab:green','tab:red','tab:gray','tab:purple','tab:cyan','gold','tab:brown']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45286679-fe4d-4d8b-9784-3699ec3d7c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function is only for co2 since that is stored in a different place\n",
    "var='co2'\n",
    "\n",
    "\n",
    "# data location\n",
    "outputdir= '/glade/u/home/aswann/work/jupyter_notebooks/esm-deck/ESM_data/'\n",
    "\n",
    "#----loop over models----#\n",
    "for m in range(len(modellist)):\n",
    "#for m in range(len(['GFDL-ESM4',  'GISS_E2.1',  'NorESM2-LM','MPI-ESM1-2-LR'])):\n",
    "    model=modellist[m]\n",
    "    print('loading model: ' +model)\n",
    "    #----loop over experiments----# \n",
    "    for r in range(len(runlist)):\n",
    "        run = runlist_wc[r]\n",
    "        print('loading run: ' +run)\n",
    "\n",
    "        print('loading variable: ' +var)\n",
    "        \n",
    "        searchpath= outputdir +model +'/' +run +'_' +var +'_*.nc'\n",
    "        #if ((model =='CESM2') or (model == 'UKESM1.2')):\n",
    "        #    # all models have a var_ filename except CESM\n",
    "        #    searchpath= outputdir +model +'/' +run +'*_' +var +'*.nc'\n",
    "        \n",
    "        filenamelist= np.sort(glob.glob(searchpath)) # sort in time order, xarray was having trouble arranging some of them in time dim\n",
    "\n",
    "        print(filenamelist)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2001e407-435c-4a48-a2bc-36b97879e273",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataco2=pd.read_csv('/glade/work/aswann/jupyter_notebooks/esm-deck/output/atmospheric_co2_flat10.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe0465b-9ea4-4202-ab49-967c83ab9b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataco2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea6803d-377d-4d1e-b595-a859eea39593",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataco2.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f9ee50-2579-4662-9c71-dc12965ea2ea",
   "metadata": {},
   "source": [
    "# Load global mean data using ben sanderson's esm-deck code for the flat10MIP paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371ee500-4783-4d17-92c9-7d56cc3911aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.patches as patches\n",
    "from scipy.stats import linregress\n",
    "\n",
    "bsdataloc='/glade/work/aswann/jupyter_notebooks/esm-deck/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b31b04f-0ac2-4079-b592-14c73d5653f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load emissions data\n",
    "\n",
    "e_flat10=pd.read_csv(bsdataloc +'flat10.csv',header=4)\n",
    "e_flat10_cdr=pd.read_csv(bsdataloc +'flat10_cdr.csv',header=4)\n",
    "e_flat10_zec=pd.read_csv(bsdataloc +'flat10_zec.csv',header=4)\n",
    "e_flat10_nz=pd.read_csv(bsdataloc +'flat10_cdr.csv',header=4)\n",
    "e_flat10_nz[150:]=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b294466-e340-4fbb-ae96-21b54d42aa0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# ESM data\n",
    "def extd(ts,leng):\n",
    "    out=np.zeros(leng)\n",
    "    if leng>len(ts):\n",
    "        out[:len(ts)]=ts.values\n",
    "        out[len(ts):]=ts.iloc[-1]\n",
    "    else:\n",
    "        out=ts[:leng]\n",
    "    return out\n",
    "#mdls=['ACCESS-ESM1-5','CESM2','GFDL-ESM4','GISS','NorESM2-LM','CNRM-ESM2-1','MPI-ESM1-2-LR']\n",
    "mdls=['ACCESS-ESM1-5','CESM2','GFDL-ESM4','GISS','NorESM2-LM','MPI-ESM1-2-LR','CNRM-ESM2-1','UKESM','UVic-ESCM-2-10','HadCM3LC-Bris']\n",
    "expt=['esm-pictrl','1pctco2','flat10','flat10-zec','flat10-cdr']\n",
    "flds=['tas','co2','fgco2','nbp','fCmissing']\n",
    "\n",
    "ed={}\n",
    "for m in mdls:\n",
    "    ed[m]={}\n",
    "    for e in expt:\n",
    "        tmpa=[]\n",
    "        for f in flds:\n",
    "                print(f)\n",
    "                fl10=bsdataloc +'ESM_data/'+m+'/'+'flat10'+'_'+f+'_'+m+'.nc'\n",
    "                fl=bsdataloc +'ESM_data/'+m+'/'+e+'_'+f+'_'+m+'.nc'\n",
    "                if os.path.exists(fl) and os.path.exists(fl10):\n",
    "                    otmp=xr.open_dataset(fl)\n",
    "                    otmp10=xr.open_dataset(fl10)\n",
    "                    if 'nep' in list(otmp.keys()):\n",
    "                        otmp=otmp.rename_vars({\"nep\": \"nbp\"})\n",
    "                    if 'nep' in list(otmp10.keys()):\n",
    "                        otmp10=otmp10.rename_vars({\"nep\": \"nbp\"})\n",
    "                    if 'CO2_GLOB' in list(otmp10.keys()):\n",
    "                        otmp10=otmp10.rename_vars({\"CO2_GLOB\": \"co2\"})\n",
    "                        otmp=otmp.rename_vars({\"CO2_GLOB\": \"co2\"})\n",
    "                    if 'NBP_GLOB' in list(otmp10.keys()):\n",
    "                        otmp10=otmp10.rename_vars({\"NBP_GLOB\": \"nbp\"})\n",
    "                        otmp=otmp.rename_vars({\"NBP_GLOB\": \"nbp\"})\n",
    "                    if 'FGCO2_GLOB' in list(otmp10.keys()):\n",
    "                        otmp10=otmp10.rename_vars({\"FGCO2_GLOB\": \"fgco2\"})\n",
    "                        otmp=otmp.rename_vars({\"FGCO2_GLOB\": \"fgco2\"})    \n",
    "                    if 'TIME' in list(otmp10.coords.keys()):                  \n",
    "                        otmp10=otmp10.swap_dims({\"TIME\": \"time\"}).rename_vars({\"TIME\": \"time\"})\n",
    "                    if 't' in list(otmp10.coords.keys()):                  \n",
    "                        otmp10=otmp10.swap_dims({\"t\": \"time\"}).rename_vars({\"t\": \"time\"})\n",
    "                    if 't' in list(otmp.coords.keys()):                  \n",
    "                        otmp=otmp.swap_dims({\"t\": \"time\"}).rename_vars({\"t\": \"time\"})\n",
    "                    if 'TIME' in list(otmp.coords.keys()):                  \n",
    "                        otmp=otmp.swap_dims({\"TIME\": \"time\"}).rename_vars({\"TIME\": \"time\"})\n",
    "                    if 'TAS_GLOB' in list(otmp10.keys()):                  \n",
    "                        otmp10=otmp10.rename_vars({\"TAS_GLOB\": \"tas\"})\n",
    "                    if 'TAS_GLOB' in list(otmp.keys()):                  \n",
    "                        otmp=otmp.rename_vars({\"TAS_GLOB\": \"tas\"})\n",
    "                        \n",
    "                    if 'temp_mm_1_5m' in list(otmp10.keys()):                  \n",
    "                        otmp10=otmp10.rename_vars({\"temp_mm_1_5m\": \"tas\"})\n",
    "                    if 'temp_mm_1_5m' in list(otmp.keys()):                  \n",
    "                        otmp=otmp.rename_vars({\"temp_mm_1_5m\": \"tas\"})\n",
    "                    \n",
    "                    if 'field1564_mm_hyb' in list(otmp10.keys()):\n",
    "                        otmp10=otmp10.rename_vars({\"field1564_mm_hyb\": \"co2\"})\n",
    "                        otmp=otmp.rename_vars({\"field1564_mm_hyb\": \"co2\"})\n",
    "                    if 'unknown' in list(otmp10.keys()):\n",
    "                        otmp10=otmp10.rename_vars({\"unknown\": \"nbp\"})\n",
    "                        otmp=otmp.rename_vars({\"unknown\": \"nbp\"})\n",
    "                   \n",
    "                    if 'field1560_mm_srf' in list(otmp10.keys()):\n",
    "                        otmp10=otmp10.rename_vars({\"field1560_mm_srf\": \"fgco2\"})\n",
    "                        otmp=otmp.rename_vars({\"field1560_mm_srf\": \"fgco2\"})\n",
    "                    if 'FCMISSING' in list(otmp.keys()):                  \n",
    "                        otmp=otmp.rename_vars({\"FCMISSING\": \"fCmissing\"})    \n",
    "                    if 'FCMISSING' in list(otmp10.keys()):                  \n",
    "                        otmp10=otmp10.rename_vars({\"FCMISSING\": \"fCmissing\"})      \n",
    "                    if '__xarray_dataarray_variable__' in list(otmp10.keys()):                  \n",
    "                        otmp10=otmp10.rename_vars({\"__xarray_dataarray_variable__\": f})\n",
    "                    if '__xarray_dataarray_variable__' in list(otmp.keys()):                  \n",
    "                        otmp=otmp.rename_vars({\"__xarray_dataarray_variable__\": f})\n",
    "\n",
    "                    tmp=otmp.copy(deep=True)\n",
    "                    tmp10=otmp10.copy(deep=True)\n",
    "                    if len(tmp.time) > 4800:\n",
    "                        tmp = tmp.isel(time=slice(0, 4800))\n",
    "                    if len(tmp10.time) > 4800:\n",
    "                        tmp10 = tmp10.isel(time=slice(0, 4800))\n",
    "                    if e=='flat10':\n",
    "                        if len(tmp.time)<1000:\n",
    "                            tmp.coords['time']=pd.date_range('1700-01-01', periods=tmp['time'].values.shape[0],freq='Y')\n",
    "                        else:\n",
    "                            tmp.coords['time']=pd.date_range('1700-01-01', periods=tmp['time'].values.shape[0],freq='M')\n",
    "                        tmp=tmp[f]    \n",
    "                    else: \n",
    "                        if len(tmp.time)<1000:\n",
    "                            tmp10.coords['time']=pd.date_range('1700-01-01', periods=tmp10['time'].values.shape[0],freq='Y')\n",
    "                            tmp.coords['time']=pd.date_range('1800-01-01', periods=tmp['time'].values.shape[0],freq='Y')\n",
    "                        else:\n",
    "                            tmp10.coords['time']=pd.date_range('1700-01-01', periods=tmp10['time'].values.shape[0],freq='M')\n",
    "                            tmp.coords['time']=pd.date_range('1800-01-01', periods=tmp['time'].values.shape[0],freq='M')\n",
    "  \n",
    "                        tmp=xr.concat((tmp10.where(tmp10['time.year'] < 1800, drop=True)[f],tmp[f]),'time')\n",
    "                    tmp=tmp.groupby('time.year').mean()\n",
    "                    if len(tmp.shape)==2:\n",
    "                        tmp=tmp.sel(lat=0).sel(lon=0).drop('lev')\n",
    "                    if len(tmp.shape)==3:\n",
    "                        tmp=tmp.sel(lat=0).sel(lon=0).drop('lat').drop('lon')\n",
    "                    if len(tmp.shape)==4:\n",
    "                        if 'lev' in list(otmp10.coords.keys()):\n",
    "                            tmp=tmp.sel(lat=0).sel(lon=0).isel(lev=[0]).drop('lat').drop('lon').drop('lev')\n",
    "                        else:\n",
    "                            tmp=tmp.sel(lat=0).sel(lon=0).isel(plev=[0]).drop('lat').drop('lon').drop('plev')\n",
    "                    \n",
    "                    if tmp.name=='tas':            \n",
    "                        t10=tmp[0:20].mean().values\n",
    "                        d10=tmp[20:40].mean().values-t10\n",
    "                        t0=t10-d10/2\n",
    "                        tmp=tmp-t0\n",
    "                    tmpa.append(tmp)\n",
    "                    if f=='fgco2' and m=='NorESM2-LM' and e=='flat10':\n",
    "                     0\n",
    "                else:\n",
    "                    print(m+e+f)\n",
    "                \n",
    "             \n",
    "        ed[m][e]=xr.merge(tmpa)\n",
    "        if len(tmpa)>0:\n",
    "            ed[m][e]['fco2fos']=ed[m][e]['tas'].copy()\n",
    "            if e=='flat10':\n",
    "                    ed[m][e]['fco2fos'].values=extd(e_flat10['Emission Rate (PgC a-1)'],len(ed[m][e]['year']))\n",
    "            if e=='flat10-cdr':\n",
    "                    ed[m][e]['fco2fos'].values=extd(e_flat10_cdr['Emission Rate (PgC a-1)'],len(ed[m][e]['year']))\n",
    "            if e=='flat10-zec':\n",
    "                    ed[m][e]['fco2fos'].values=extd(e_flat10_zec['Emission Rate (PgC a-1)'],len(ed[m][e]['year']))\n",
    "            if m=='ACCESS-ESM1-5':\n",
    "                ed[m][e]['co2']=ed[m][e]['co2']*1e6\n",
    "\n",
    "            if m=='GFDL-ESM4':\n",
    "                ed[m][e]['co2']=ed[m][e]['co2'].isel(lev=0)*1e6\n",
    "                #ed[m][e]['nbp']=ed[m][e]['nbp']*5.1e14/1e12*3600*24*365\n",
    "                aco2=np.diff(ed[m][e]['co2'][:],append=ed[m][e]['co2'][-1])*2.12\n",
    "                ln=len(ed[m][e]['fco2fos'])\n",
    "                ed[m][e]['fgco2']=ed[m][e]['fgco2'][:ln]\n",
    "                ed[m][e]['nbp']=ed[m][e]['fco2fos'][:ln]-aco2[:ln]-ed[m][e]['fgco2'][:ln]\n",
    "\n",
    "            if m=='CESM2':\n",
    "                ed[m][e]['fgco2']=ed[m][e]['fgco2']/1e15\n",
    "                ed[m][e]['nbp']=ed[m][e]['nbp']/1e15\n",
    "                \n",
    "                ed[m][e]['co2']=280+np.cumsum(ed[m][e]['fco2fos']-ed[m][e]['fgco2']-ed[m][e]['nbp'])/2.12\n",
    "            \n",
    "                ed[m][e]['co2']=280+np.cumsum(ed[m][e]['fco2fos']-ed[m][e]['fgco2']-ed[m][e]['nbp'])/2.12\n",
    "            if m=='NorESM2-LM':\n",
    "                ed[m][e]['co2']=ed[m][e]['co2']*1e6\n",
    "                #ed[m][e]['nbp']=ed[m][e]['nbp']#*5.1e14/1e12*3600*24*365\n",
    "                aco2=np.diff(ed[m][e]['co2'][:],append=ed[m][e]['co2'][-1])*2.12\n",
    "                ed[m][e]['nbp']=ed[m][e]['fco2fos']-aco2-ed[m][e]['fgco2']\n",
    "                #ed[m][e]['fgco2']=ed[m][e]['fgco2']*5.1e14/1e12*3600*24*365\n",
    "            if m=='UKESM':\n",
    "                ed[m][e]['nbp']=ed[m][e]['nbp']*5.1e14/1e12*.29\n",
    "                #ed[m][e]['nbp']=ed[m][e]['nbp']*5.1e14/1e12*3600*24*365\n",
    "                #ed[m][e]['fgco2']=ed[m][e]['fgco2']*5.1e14/1e12*3600*24*365*.71\n",
    "            if m=='CNRM-ESM2-1':\n",
    "                ed[m][e]['fgco2']=ed[m][e]['fgco2']-ed[m][e]['fCmissing']\n",
    "                #ed[m][e]['nbp']=ed[m][e]['nbp']*5.1e14/1e12*3600*24*365\n",
    "                #ed[m][e]['fgco2']=ed[m][e]['fgco2']*5.1e14/1e12*3600*24*365*.71\n",
    "            if m=='HadCM3LC-Bris':\n",
    "                0#ed[m][e]['fgco2']=ed[m][e]['fgco2']*5.1e14/1e12*.71\n",
    "                ed[m][e]['nbp']=ed[m][e]['nbp']\n",
    "                \n",
    "\n",
    "\n",
    "ed[\"GFDL-ESM4\"]['flat10']['fgco2'][0] = ed[\"GFDL-ESM4\"]['flat10']['fgco2'][0]-10        # correct first ocean value of GFDL-ESM4\n",
    "ed[\"GFDL-ESM4\"]['flat10-zec']['fgco2'][0] = ed[\"GFDL-ESM4\"]['flat10-zec']['fgco2'][0]-10        # correct first ocean value of GFDL-ESM4\n",
    "ed[\"GFDL-ESM4\"]['flat10-cdr']['fgco2'][0] = ed[\"GFDL-ESM4\"]['flat10-cdr']['fgco2'][0]-10        # correct first ocean value of GFDL-ESM4\n",
    "\n",
    "for i,m in enumerate(mdls):\n",
    "    print(m)\n",
    "    try:\n",
    "        ed[m]['flat10-cdr']['C_atm']=(ed[m]['flat10-cdr']['co2']-ed[m]['flat10']['co2'][0])*2.13\n",
    "    except:\n",
    "        print('fail cdr')\n",
    "    try:\n",
    "        ed[m]['flat10-zec']['C_atm']=(ed[m]['flat10-zec']['co2']-ed[m]['flat10']['co2'][0])*2.13\n",
    "    except:\n",
    "        print('fail zec')\n",
    "    try:\n",
    "        ed[m]['flat10']['C_atm']=(ed[m]['flat10']['co2']-ed[m]['flat10']['co2'][0])*2.13\n",
    "\n",
    "    except:\n",
    "        print('fail flat')    \n",
    "\n",
    "\n",
    "t0mat = {}\n",
    "for i, m in enumerate(mdls):\n",
    "    # Extract the first 30 years of the temperature timeseries\n",
    "    x = np.arange(0, 30)  # Years 0 to 29\n",
    "    y = np.array(ed[m]['flat10']['tas'][:30])  # Corresponding temperature values\n",
    "\n",
    "    # Perform linear regression to estimate temperature at time zero\n",
    "    slope, intercept, _, _, _ = linregress(x, y)\n",
    "    t0mat[m] = intercept  # Use the intercept as the temperature at time zero\n",
    "\n",
    "    # Adjust the temperature timeseries for each experiment\n",
    "    ed[m]['flat10-cdr']['tas'] = ed[m]['flat10-cdr']['tas'] - intercept\n",
    "    ed[m]['flat10-zec']['tas'] = ed[m]['flat10-zec']['tas'] - intercept\n",
    "    ed[m]['flat10']['tas'] = ed[m]['flat10']['tas'] - intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f774662-895a-4c46-bc17-003f7f06d8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "ed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf80ea8-4c3c-4370-944d-c44762342bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#can save to csv files like this\n",
    "\n",
    "# Initialize empty dictionaries to store data for each model\n",
    "data_a = {}\n",
    "data_d = {}\n",
    "data_g = {}\n",
    "data_j = {}\n",
    "\n",
    "# Iterate over models and dynamically handle timeseries lengths\n",
    "for model in mdls:\n",
    "    # Get the length of the timeseries for the current model\n",
    "    length = len(ed[model]['flat10']['tas'])\n",
    "\n",
    "    # Extract data for the available length\n",
    "    data_a[model] = ed[model]['flat10']['tas'].values[:length]\n",
    "    data_d[model] = ed[model]['flat10']['co2'].values[:length]\n",
    "    data_g[model] = np.cumsum(ed[model]['flat10']['nbp'].values[:length])\n",
    "    data_j[model] = np.cumsum(ed[model]['flat10']['fgco2'].values[:length])\n",
    "\n",
    "# Convert dictionaries to DataFrames\n",
    "df_a = pd.DataFrame.from_dict(data_a, orient='index').transpose()\n",
    "df_d = pd.DataFrame.from_dict(data_d, orient='index').transpose()\n",
    "df_g = pd.DataFrame.from_dict(data_g, orient='index').transpose()\n",
    "df_j = pd.DataFrame.from_dict(data_j, orient='index').transpose()\n",
    "\n",
    "# Save the DataFrames to CSV files\n",
    "df_a.to_csv(\"output/temperatures_flat10.csv\", index=False)\n",
    "df_d.to_csv(\"output/atmospheric_co2_flat10.csv\", index=False)\n",
    "df_g.to_csv(\"output/land_carbon_sink_flat10.csv\", index=False)\n",
    "df_j.to_csv(\"output/ocean_carbon_sink_flat10.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed5565d-9768-4f21-b68e-199c6d1c8597",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6857018d-ac45-4c8d-b537-51bd29410aa8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCESS-ESM1-5 getting grid info\n",
      "CESM2 getting grid info\n",
      "GFDL-ESM4 getting grid info\n",
      "GISS_E2.1 getting grid info\n",
      "NorESM2-LM getting grid info\n",
      "MPI-ESM1-2-LR getting grid info\n",
      "CNRM-ESM2-1 getting grid info\n",
      "HadCM3LC-Bris getting grid info\n",
      "UKESM1.2 getting grid info\n",
      "processing model: ACCESS-ESM1-5\n",
      "processing run: flat10\n",
      "processing variable: cVeg\n",
      "loading variable: cVeg\n",
      "finished loading ACCESS-ESM1-5 *lat10 cVeg\n",
      "processing variable: cSoil\n",
      "loading variable: cSoil\n",
      "finished loading ACCESS-ESM1-5 *lat10 cSoil\n",
      "processing variable: cLitter\n",
      "loading variable: cLitter\n",
      "finished loading ACCESS-ESM1-5 *lat10 cLitter\n",
      "processing variable: nbp\n",
      "loading variable: nbp\n",
      "finished loading ACCESS-ESM1-5 *lat10 nbp\n",
      "processing variable: gpp\n",
      "loading variable: gpp\n",
      "finished loading ACCESS-ESM1-5 *lat10 gpp\n",
      "processing variable: rh\n",
      "loading variable: rh\n",
      "finished loading ACCESS-ESM1-5 *lat10 rh\n",
      "processing variable: tas\n",
      "loading variable: tas\n",
      "finished loading ACCESS-ESM1-5 *lat10 tas\n",
      "processing variable: pr\n",
      "loading variable: pr\n",
      "finished loading ACCESS-ESM1-5 *lat10 pr\n",
      "processing run: flat10_zec\n",
      "processing variable: cVeg\n",
      "loading variable: cVeg\n",
      "finished loading ACCESS-ESM1-5 *zec cVeg\n",
      "processing variable: cSoil\n",
      "loading variable: cSoil\n",
      "finished loading ACCESS-ESM1-5 *zec cSoil\n",
      "processing variable: cLitter\n",
      "loading variable: cLitter\n",
      "finished loading ACCESS-ESM1-5 *zec cLitter\n",
      "processing variable: nbp\n",
      "loading variable: nbp\n",
      "finished loading ACCESS-ESM1-5 *zec nbp\n",
      "processing variable: gpp\n",
      "loading variable: gpp\n",
      "finished loading ACCESS-ESM1-5 *zec gpp\n",
      "processing variable: rh\n",
      "loading variable: rh\n",
      "finished loading ACCESS-ESM1-5 *zec rh\n",
      "processing variable: tas\n",
      "loading variable: tas\n",
      "finished loading ACCESS-ESM1-5 *zec tas\n",
      "processing variable: pr\n",
      "loading variable: pr\n",
      "finished loading ACCESS-ESM1-5 *zec pr\n",
      "processing run: flat10_cdr\n",
      "processing variable: cVeg\n",
      "loading variable: cVeg\n",
      "finished loading ACCESS-ESM1-5 *cdr cVeg\n",
      "processing variable: cSoil\n",
      "loading variable: cSoil\n",
      "finished loading ACCESS-ESM1-5 *cdr cSoil\n",
      "processing variable: cLitter\n",
      "loading variable: cLitter\n",
      "finished loading ACCESS-ESM1-5 *cdr cLitter\n",
      "processing variable: nbp\n",
      "loading variable: nbp\n",
      "finished loading ACCESS-ESM1-5 *cdr nbp\n",
      "processing variable: gpp\n",
      "loading variable: gpp\n",
      "finished loading ACCESS-ESM1-5 *cdr gpp\n",
      "processing variable: rh\n",
      "loading variable: rh\n",
      "finished loading ACCESS-ESM1-5 *cdr rh\n",
      "processing variable: tas\n",
      "loading variable: tas\n",
      "finished loading ACCESS-ESM1-5 *cdr tas\n",
      "processing variable: pr\n",
      "loading variable: pr\n",
      "finished loading ACCESS-ESM1-5 *cdr pr\n",
      "processing model: CESM2\n",
      "processing run: flat10\n",
      "processing variable: cVeg\n",
      "loading variable: cVeg\n",
      "finished loading CESM2 *lat10 cVeg\n",
      "processing variable: cSoil\n",
      "loading variable: cSoil\n",
      "finished loading CESM2 *lat10 cSoil\n",
      "processing variable: cLitter\n",
      "loading variable: cLitter\n",
      "finished loading CESM2 *lat10 cLitter\n",
      "processing variable: nbp\n",
      "loading variable: nbp\n",
      "finished loading CESM2 *lat10 nbp\n",
      "processing variable: gpp\n",
      "loading variable: gpp\n",
      "finished loading CESM2 *lat10 gpp\n",
      "processing variable: rh\n",
      "loading variable: rh\n",
      "finished loading CESM2 *lat10 rh\n",
      "processing variable: tas\n",
      "loading variable: tas\n",
      "finished loading CESM2 *lat10 tas\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 218\u001b[39m\n\u001b[32m    216\u001b[39m     C_highlat_mat[\u001b[32m0\u001b[39m:\u001b[38;5;28mlen\u001b[39m(C_global),m,e,v]= C_highlat\n\u001b[32m    217\u001b[39m     C_troplat_mat[\u001b[32m0\u001b[39m:\u001b[38;5;28mlen\u001b[39m(C_global),m,e,v]= C_troplat\n\u001b[32m--> \u001b[39m\u001b[32m218\u001b[39m     \u001b[43mC_midlat_mat\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mC_global\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43mv\u001b[49m\u001b[43m]\u001b[49m= C_midlat\n\u001b[32m    220\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m: \u001b[38;5;66;03m# it is a carbon variable and we want to make a sum\u001b[39;00m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# total carbon on land. Becuase it is in units of carbon/area (kgC/m2), multiply by area\u001b[39;00m\n\u001b[32m    222\u001b[39m     \u001b[38;5;66;03m# our area variable is in m2\u001b[39;00m\n\u001b[32m    223\u001b[39m     C_global =(((data_var*landarea)).sum(dim=[\u001b[33m'\u001b[39m\u001b[33mlat\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33mlon\u001b[39m\u001b[33m'\u001b[39m]))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/glade/u/apps/opt/conda/envs/npl-2025b/lib/python3.12/site-packages/xarray/core/common.py:181\u001b[39m, in \u001b[36mAbstractArray.__array__\u001b[39m\u001b[34m(self, dtype, copy)\u001b[39m\n\u001b[32m    179\u001b[39m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m    180\u001b[39m             copy = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m181\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m np.array(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m, dtype=dtype, copy=copy)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/glade/u/apps/opt/conda/envs/npl-2025b/lib/python3.12/site-packages/xarray/core/dataarray.py:797\u001b[39m, in \u001b[36mDataArray.values\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    784\u001b[39m \u001b[38;5;129m@property\u001b[39m\n\u001b[32m    785\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mvalues\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> np.ndarray:\n\u001b[32m    786\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    787\u001b[39m \u001b[33;03m    The array's data converted to numpy.ndarray.\u001b[39;00m\n\u001b[32m    788\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    795\u001b[39m \u001b[33;03m    to this array may be reflected in the DataArray as well.\u001b[39;00m\n\u001b[32m    796\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m797\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mvariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/glade/u/apps/opt/conda/envs/npl-2025b/lib/python3.12/site-packages/xarray/core/variable.py:536\u001b[39m, in \u001b[36mVariable.values\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    533\u001b[39m \u001b[38;5;129m@property\u001b[39m\n\u001b[32m    534\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mvalues\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> np.ndarray:\n\u001b[32m    535\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"The variable's data as a numpy.ndarray\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m536\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_as_array_or_item\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_data\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/glade/u/apps/opt/conda/envs/npl-2025b/lib/python3.12/site-packages/xarray/core/variable.py:316\u001b[39m, in \u001b[36m_as_array_or_item\u001b[39m\u001b[34m(data)\u001b[39m\n\u001b[32m    302\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_as_array_or_item\u001b[39m(data):\n\u001b[32m    303\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Return the given values as a numpy array, or as an individual item if\u001b[39;00m\n\u001b[32m    304\u001b[39m \u001b[33;03m    it's a 0d datetime64 or timedelta64 array.\u001b[39;00m\n\u001b[32m    305\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    314\u001b[39m \u001b[33;03m    TODO: remove this (replace with np.asarray) once these issues are fixed\u001b[39;00m\n\u001b[32m    315\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m316\u001b[39m     data = \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    317\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m data.ndim == \u001b[32m0\u001b[39m:\n\u001b[32m    318\u001b[39m         kind = data.dtype.kind\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/glade/u/apps/opt/conda/envs/npl-2025b/lib/python3.12/site-packages/dask/array/core.py:1729\u001b[39m, in \u001b[36mArray.__array__\u001b[39m\u001b[34m(self, dtype, copy, **kwargs)\u001b[39m\n\u001b[32m   1722\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m copy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[32m   1723\u001b[39m     warnings.warn(\n\u001b[32m   1724\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mCan\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt acquire a memory view of a Dask array. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1725\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mThis will raise in the future.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1726\u001b[39m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[32m   1727\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1729\u001b[39m x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1731\u001b[39m \u001b[38;5;66;03m# Apply requested dtype and convert non-numpy backends to numpy.\u001b[39;00m\n\u001b[32m   1732\u001b[39m \u001b[38;5;66;03m# If copy is True, numpy is going to perform its own deep copy\u001b[39;00m\n\u001b[32m   1733\u001b[39m \u001b[38;5;66;03m# after this method returns.\u001b[39;00m\n\u001b[32m   1734\u001b[39m \u001b[38;5;66;03m# If copy is None, finalize() ensures that the returned object\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;66;03m# does not share memory with an object stored in the graph or on a\u001b[39;00m\n\u001b[32m   1736\u001b[39m \u001b[38;5;66;03m# process-local Worker.\u001b[39;00m\n\u001b[32m   1737\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m np.asarray(x, dtype=dtype)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/glade/u/apps/opt/conda/envs/npl-2025b/lib/python3.12/site-packages/dask/base.py:373\u001b[39m, in \u001b[36mDaskMethodsMixin.compute\u001b[39m\u001b[34m(self, **kwargs)\u001b[39m\n\u001b[32m    349\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute\u001b[39m(\u001b[38;5;28mself\u001b[39m, **kwargs):\n\u001b[32m    350\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Compute this dask collection\u001b[39;00m\n\u001b[32m    351\u001b[39m \n\u001b[32m    352\u001b[39m \u001b[33;03m    This turns a lazy Dask collection into its in-memory equivalent.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    371\u001b[39m \u001b[33;03m    dask.compute\u001b[39;00m\n\u001b[32m    372\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m373\u001b[39m     (result,) = \u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraverse\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    374\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/glade/u/apps/opt/conda/envs/npl-2025b/lib/python3.12/site-packages/dask/base.py:681\u001b[39m, in \u001b[36mcompute\u001b[39m\u001b[34m(traverse, optimize_graph, scheduler, get, *args, **kwargs)\u001b[39m\n\u001b[32m    678\u001b[39m     expr = expr.optimize()\n\u001b[32m    679\u001b[39m     keys = \u001b[38;5;28mlist\u001b[39m(flatten(expr.__dask_keys__()))\n\u001b[32m--> \u001b[39m\u001b[32m681\u001b[39m     results = \u001b[43mschedule\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexpr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    683\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m repack(results)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/glade/u/apps/opt/conda/envs/npl-2025b/lib/python3.12/queue.py:171\u001b[39m, in \u001b[36mQueue.get\u001b[39m\u001b[34m(self, block, timeout)\u001b[39m\n\u001b[32m    169\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    170\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._qsize():\n\u001b[32m--> \u001b[39m\u001b[32m171\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnot_empty\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    172\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m timeout < \u001b[32m0\u001b[39m:\n\u001b[32m    173\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m\u001b[33mtimeout\u001b[39m\u001b[33m'\u001b[39m\u001b[33m must be a non-negative number\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/glade/u/apps/opt/conda/envs/npl-2025b/lib/python3.12/threading.py:355\u001b[39m, in \u001b[36mCondition.wait\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    353\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[32m    354\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m355\u001b[39m         \u001b[43mwaiter\u001b[49m\u001b[43m.\u001b[49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    356\u001b[39m         gotit = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    357\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# ## Create a netcdf file with a matrix of processed time series\n",
    "# this works with environment npl2025b\n",
    "# to run on the command line:\n",
    "#\n",
    "# module load conda\n",
    "# conda activate npl-2025b\n",
    "# python create_metrics_matrix.py\n",
    "\n",
    "\n",
    "# This is a script version of a python notebook\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import numpy.matlib\n",
    "import numpy.ma as ma\n",
    "\n",
    "import xarray as xr\n",
    "\n",
    "\n",
    "import time\n",
    "import cftime\n",
    "import netCDF4 as nc\n",
    "from datetime import timedelta\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import glob\n",
    "\n",
    "\n",
    "import nc_time_axis\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy\n",
    "\n",
    "\n",
    "import warnings\n",
    "#warnings.filterwarnings(\"ignore\", message=\"invalid value encountered in divide\")\n",
    "\n",
    "# --- custom files\n",
    "# load custom functions for analyzing flat10\n",
    "\n",
    "from loading_function_flat10 import load_flat10, load_one_model, load_one_model_onevar, load_grid, select_time_slice, weighted_temporal_mean \n",
    "\n",
    "\n",
    "# ###-------------####\n",
    "# define which runs and models\n",
    "\n",
    "outputdir= '/glade/campaign/cgd/tss/people/aswann/flat10/'\n",
    "\n",
    "modellist_orig= ['ACCESS-ESM1-5',  \n",
    "            'CESM2',    \n",
    "            'GFDL-ESM4',  \n",
    "            'GISS_E2.1',  \n",
    "            'NorESM2-LM',\n",
    "            'MPI-ESM1-2-LR',\n",
    "            'CNRM-ESM2-1',\n",
    "            'HadCM3LC-Bris',\n",
    "            'UKESM1.2']\n",
    "modellist=modellist_orig\n",
    "\n",
    "runlist = ['flat10','flat10_zec','flat10_cdr']\n",
    "# use a wildcard to capture different ways the folders and runs are named across models\n",
    "runlist_wc = ['*lat10','*zec','*cdr']\n",
    "\n",
    "varlist_load=['cVeg','cSoil','cLitter','nbp','gpp','rh','tas','pr'] #, 'gpp','fgco2', 'ra', 'rh']#, 'npp'] # not working beyond nbp for norESM\n",
    "varlist_analyze=['cVeg','cSoil','cTot','cLitter','nbp','gpp','rh','tas','pr']\n",
    "varlist=varlist_load\n",
    "unitslist=['kgC m-2','kgC m-2','kgC m-2','kgC m-2 s-1','kgC m-2 s-1','kgC m-2 s-1','K','kg m-2 s-1']\n",
    "\n",
    "\n",
    "# global and three latitude bands: trop, mid, high\n",
    "latlist=['global','highlat','troplat','midlat']\n",
    "troplat=30\n",
    "highlat=60\n",
    "\n",
    "## unit conversions\n",
    "#unitconversions\n",
    "sperday=60*60*24\n",
    "speryr=60*60*24*365\n",
    "\n",
    "PgperKg = 1e-12\n",
    "# to convert kg m-2 s-1 to kg/m2/yr multiply by speryr\n",
    "\n",
    "# initialize a dictionary to hold all of the data\n",
    "data_dict={}\n",
    "\n",
    "\n",
    "# ###---------------####\n",
    "\n",
    "#-- load grid info\n",
    "data_dict = load_grid(data_dict,modellist)\n",
    "\n",
    "\n",
    "#-- load data\n",
    "#data_dict=load_flat10(data_dict, modellist, runlist, runlist_wc, varlist)\n",
    "\n",
    "#-- toggle to analysis lists\n",
    "modellist=modellist_orig\n",
    "varlist=varlist_load\n",
    "\n",
    "\n",
    "# ###---------------####\n",
    "# create matrix of zonal mean time series for \n",
    "\n",
    "#- initialize\n",
    "C_global_mat= np.empty([350,len(modellist),len(runlist),len(varlist)])\n",
    "C_highlat_mat= np.empty([350,len(modellist),len(runlist),len(varlist)])\n",
    "C_troplat_mat= np.empty([350,len(modellist),len(runlist),len(varlist)])\n",
    "C_midlat_mat= np.empty([350,len(modellist),len(runlist),len(varlist)])\n",
    "\n",
    "# create a time series of years for the first dimension\n",
    "ts= np.arange(350)\n",
    "\n",
    "for m in range(len(modellist)):\n",
    "#for m in range(len(['GFDL-ESM4',  'GISS_E2.1',  'NorESM2-LM','MPI-ESM1-2-LR'])):\n",
    "    model=modellist[m]\n",
    "    print('processing model: ' +model)\n",
    "\n",
    "    # get area and landfrac from the dictionary where they have been pre-loaded\n",
    "    ds_area = data_dict[modellist[m] +'_' +'areacella']\n",
    "    ds_landfrac = data_dict[modellist[m] +'_' +'landfrac']\n",
    "    \n",
    "    #----loop over experiments----# \n",
    "    for e in range(len(runlist)):\n",
    "        run = runlist[e]\n",
    "        print('processing run: ' +run)\n",
    "\n",
    "        #ds=load_one_model(model,runlist_wc[e],varlist)\n",
    "        #ds=data_dict[modellist[m] +'_' +runlist[e]]\n",
    "\n",
    "\n",
    "        #----loop over variables----#\n",
    "        for v in range(len(varlist)):\n",
    "            var=varlist[v]\n",
    "            print('processing variable: ' +var)\n",
    "\n",
    "            ds=load_one_model_onevar(model,runlist_wc[e],var)\n",
    "\n",
    "\n",
    "            if model=='CESM2':\n",
    "                area = ds_area['areacella'].squeeze().reindex_like(ds, method='nearest',tolerance=0.05)\n",
    "            else:\n",
    "                area = ds_area['areacella'].reindex_like(ds, method='nearest',tolerance=0.05)\n",
    "            \n",
    "            landfrac=ds_landfrac['sftlf'].reindex_like(ds, method='nearest',tolerance=0.05)\n",
    "            \n",
    "            if landfrac.max(dim=['lat','lon'])>1: #test if landfrac is on a 0-100 or 0-1 scale\n",
    "                landfrac=landfrac/100\n",
    "                \n",
    "            landarea=area*landfrac\n",
    "             \n",
    "\n",
    "            # NorESM has drift that needs to be corrected\n",
    "            # load the drift correction matrix and remove the drift\n",
    "            if model=='NorESM2-LM':\n",
    "                if var=='cVeg':\n",
    "                    field = pickle.load(open('/glade/campaign/cgd/tss/people/aswann/flat10/NorESM2-LM/NorESM2-LM_2D_TOTVEGC_ann_drift.pkl','rb'))\n",
    "                    adj_matrix = xr.DataArray(np.squeeze(field), dims=['lat','lon'], coords={'latitude': ds.lat, 'longitude':ds.lon})##,unit={'g C m-2 yr-1'})\n",
    "                    ty=ds['time'].dt.year\n",
    "                    tyindx=ty-ty[0]+1\n",
    "                    adjustment = adj_matrix* tyindx*(1/1000) #this is the drift for each time point and each gridcell in kg C m-2 yr-1\n",
    "\n",
    "                    ds[var]=ds[var]+adjustment # remove the drift from the variable\n",
    "                    \n",
    "                elif var=='cSoil':\n",
    "                    field = pickle.load(open('/glade/campaign/cgd/tss/people/aswann/flat10/NorESM2-LM/NorESM2-LM_2D_TOTSOMC_ann_drift.pkl','rb'))\n",
    "                    adj_matrix = xr.DataArray(np.squeeze(field), dims=['lat','lon'], coords={'latitude': ds.lat, 'longitude':ds.lon})##,unit={'g C m-2 yr-1'})\n",
    "                    ty=ds['time'].dt.year\n",
    "                    tyindx=ty-ty[0]+1\n",
    "                    adjustment = adj_matrix* tyindx*(1/1000) #this is the drift for each time point and each gridcell in kg C m-2 yr-1\n",
    "\n",
    "                    ds[var]=ds[var]+adjustment # remove the drift from the variable\n",
    "                    \n",
    "                elif var=='cLitter':\n",
    "                    field = pickle.load(open('/glade/campaign/cgd/tss/people/aswann/flat10/NorESM2-LM/NorESM2-LM_2D_TOTLITC_ann_drift.pkl','rb'))\n",
    "                    adj_matrix = xr.DataArray(np.squeeze(field), dims=['lat','lon'], coords={'latitude': ds.lat, 'longitude':ds.lon})##,unit={'g C m-2 yr-1'})\n",
    "                    ty=ds['time'].dt.year\n",
    "                    tyindx=ty-ty[0]+1\n",
    "                    adjustment = adj_matrix* tyindx*(1/1000) #this is the drift for each time point and each gridcell in kg C m-2 yr-1\n",
    "\n",
    "                    ds[var]=ds[var]+adjustment # remove the drift from the variable\n",
    "\n",
    "            data_var= weighted_temporal_mean(ds, var)\n",
    "\n",
    "            # mask for nans \n",
    "            # Mask landarea where it's zero or NaN to avoid invalid values\n",
    "            valid_mask = (landarea > 0) & landarea.notnull()\n",
    "            masked_landarea = landarea.where(valid_mask)\n",
    "            masked_data = data_var.where(valid_mask)\n",
    "\n",
    "            landarea_global = masked_landarea.sum(dim=['lat','lon'])\n",
    "            #landarea_global = landarea_global.where(landarea_global != 0)\n",
    "            landarea_highlat = ((masked_landarea.where(ds.lat>=highlat)).sum(dim=['lat','lon']))\n",
    "            #landarea_highlat = landarea_highlat.where(landarea_highlat != 0)\n",
    "            landarea_troplat = ((masked_landarea.where((ds.lat>=-troplat) & (ds.lat<=troplat))).sum(dim=['lat','lon']))\n",
    "            #landarea_troplat = landarea_troplat.where(landarea_troplat != 0)\n",
    "            landarea_midlat = ((masked_landarea.where((ds.lat>=troplat) & (ds.lat<=highlat))).sum(dim=['lat','lon']))\n",
    "            #landarea_midlat = landarea_midlat.where(landarea_midlat != 0)\n",
    "\n",
    "            if var=='tas' or var=='pr': \n",
    "                # if this is *not* a carbon variable then we want to make an average\n",
    "                # C_global =(((data_var*landarea)).sum(dim=['lat','lon']))/landarea.sum(dim=['lat','lon'])\n",
    "                # C_highlat=(((data_var*landarea).where(ds.lat>=highlat)).sum(dim=['lat','lon']))/((landarea.where(ds.lat>=highlat)).sum(dim=['lat','lon']))\n",
    "                # C_troplat=(((data_var*landarea).where((ds.lat>=-troplat) & (ds.lat<=troplat))).sum(dim=['lat','lon']))/((landarea.where((ds.lat>=-troplat) & (ds.lat<=troplat))).sum(dim=['lat','lon']))\n",
    "                # C_midlat=(((data_var*landarea).where((ds.lat>=troplat) & (ds.lat<=highlat))).sum(dim=['lat','lon']))/((landarea.where((ds.lat>=troplat) & (ds.lat<=highlat))).sum(dim=['lat','lon']))\n",
    "\n",
    "                C_global =(((data_var*landarea)).sum(dim=['lat','lon']))/landarea_global\n",
    "                C_highlat=(((data_var*landarea).where(ds.lat>=highlat)).sum(dim=['lat','lon']))/landarea_highlat\n",
    "                C_troplat=(((data_var*landarea).where((ds.lat>=-troplat) & (ds.lat<=troplat))).sum(dim=['lat','lon']))/landarea_troplat\n",
    "                C_midlat=(((data_var*landarea).where((ds.lat>=troplat) & (ds.lat<=highlat))).sum(dim=['lat','lon']))/landarea_midlat\n",
    "    \n",
    "                #put into matrix \n",
    "                C_global_mat[0:len(C_global),m,e,v]= C_global\n",
    "                C_highlat_mat[0:len(C_global),m,e,v]= C_highlat\n",
    "                C_troplat_mat[0:len(C_global),m,e,v]= C_troplat\n",
    "                C_midlat_mat[0:len(C_global),m,e,v]= C_midlat\n",
    "            \n",
    "            else: # it is a carbon variable and we want to make a sum\n",
    "                # total carbon on land. Becuase it is in units of carbon/area (kgC/m2), multiply by area\n",
    "                # our area variable is in m2\n",
    "                C_global =(((data_var*landarea)).sum(dim=['lat','lon']))\n",
    "                C_highlat=((data_var*landarea).where(ds.lat>=highlat)).sum(dim=['lat','lon'])\n",
    "                C_troplat=((data_var*landarea).where((ds.lat>=-troplat) & (ds.lat<=troplat))).sum(dim=['lat','lon'])\n",
    "                C_midlat=((data_var*landarea).where((ds.lat>=troplat) & (ds.lat<=highlat))).sum(dim=['lat','lon'])\n",
    "    \n",
    "                #put into matrix and convert to PgC (kgC => PgC, divide by 10^12)\n",
    "                C_global_mat[0:len(C_global),m,e,v]= C_global*PgperKg\n",
    "                C_highlat_mat[0:len(C_global),m,e,v]= C_highlat*PgperKg\n",
    "                C_troplat_mat[0:len(C_global),m,e,v]= C_troplat*PgperKg\n",
    "                C_midlat_mat[0:len(C_global),m,e,v]= C_midlat*PgperKg\n",
    "\n",
    "            # reset values after the end of the time series to nan\n",
    "            C_global_mat[(len(C_global)):,m,e,v]=np.nan\n",
    "            C_highlat_mat[(len(C_highlat)):,m,e,v]=np.nan\n",
    "            C_troplat_mat[(len(C_troplat)):,m,e,v]=np.nan\n",
    "            C_midlat_mat[(len(C_midlat)):,m,e,v]=np.nan\n",
    "\n",
    "        del ds # remove the dataset from memory\n",
    "        del data_var # remove from memory\n",
    "\n",
    "# ###----------------####\n",
    "\n",
    "# put the matrix into an xarray dataset\n",
    "data_array_combined = np.full((len(ts), len(modellist), len(runlist), len(varlist), len(latlist)),np.nan)\n",
    "\n",
    "data_array_combined[:,:,:,:,0]=C_global_mat\n",
    "data_array_combined[:,:,:,:,1]=C_highlat_mat\n",
    "data_array_combined[:,:,:,:,2]=C_troplat_mat\n",
    "data_array_combined[:,:,:,:,3]=C_midlat_mat\n",
    "\n",
    "\n",
    "# ###----------------####\n",
    "# put into an xarray dataset\n",
    "\n",
    "ds_C_global= xr.Dataset(\n",
    "    {\n",
    "        \"data\": ([\"time\", \"model\", \"run\", \"var\",\"latrange\"], data_array_combined)\n",
    "    },\n",
    "    coords={\n",
    "        \"time\": ts,\n",
    "        \"model\": modellist,\n",
    "        \"run\": runlist,\n",
    "        \"var\": varlist,\n",
    "        \"latrange\": latlist\n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "# ###----------------####\n",
    "\n",
    "# - save the matrix to a netcdf file\n",
    "##ds_C_global.to_netcdf(\"C_metrics_matrix.nc\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "736bd532-6a05-453c-a989-d3311abf6511",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f8d425-8bf4-48ec-bb4f-fc4fbee38c28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca0502e-37af-4b00-8cc6-5a14de37dd60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f323eb68-6046-4e54-8578-aebb3cdf7aad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42a73ff3-6569-4941-b620-a68b7a869e73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/glade/campaign/cgd/tss/people/aswann/flat10/ACCESS-ESM1-5/flat10_cdr/*cLitter_*.nc'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "searchpath= outputdir +model +'/' +run +'/*' +var +'_*.nc'\n",
    "searchpath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f16de35-1f0d-42e2-ba20-1f5cee2b76f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], dtype=float64)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filenamelist= np.sort(glob.glob(searchpath))\n",
    "filenamelist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34a2467-8ba7-499a-9fdd-26d23d25aa4a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NPL 2025b",
   "language": "python",
   "name": "npl-2025b"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
